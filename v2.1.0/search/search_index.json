{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Formula Student Driverless Simulator Welcome to the FSDS documentation. This home page contains an index with a brief description of the different sections in the documentation. Feel free to read in whatever order preferred. In any case, here are a few suggestions for newcomers. Get familiar with the architecture. The system overview introduces you to the ideas and concepts of the system. Launch the simulator. Follow the getting started guide to get the simulation up and running. Connect your autonomous system. Either use the ROS bridge or Python client Looking for details on FSOnline 2020? Please visit version v1.4.1 for a the code used during FS-Online. That version also includes the integration guide describing the rules regarding sensors of FSOnline 2020.","title":"Home"},{"location":"#formula-student-driverless-simulator","text":"Welcome to the FSDS documentation. This home page contains an index with a brief description of the different sections in the documentation. Feel free to read in whatever order preferred. In any case, here are a few suggestions for newcomers. Get familiar with the architecture. The system overview introduces you to the ideas and concepts of the system. Launch the simulator. Follow the getting started guide to get the simulation up and running. Connect your autonomous system. Either use the ROS bridge or Python client","title":"Formula Student Driverless Simulator"},{"location":"#looking-for-details-on-fsonline-2020","text":"Please visit version v1.4.1 for a the code used during FS-Online. That version also includes the integration guide describing the rules regarding sensors of FSOnline 2020.","title":"Looking for details on FSOnline 2020?"},{"location":"camera/","text":"Cameras You know what camera's are. There are two camera types: RGB camera captures color images, just like any normal normal video camera. Depth Cameras (aka DepthPerspective) act as follows: each pixel is given a float value in meters corresponding to the smallest distance from the camera to that point. It captures the world in 3d! At the moment there is no added noise to the images. Add a camera to the car To add a camera to your vehicle, add the following json to the Cameras map in your settings.json : \"Camera1\": { \"CaptureSettings\": [{ \"ImageType\": 0, \"Width\": 785, \"Height\": 785, \"FOV_Degrees\": 90 }], \"X\": 1.0, \"Y\": 0.06, \"Z\": -2.20, \"Pitch\": 0.0, \"Roll\": 0.0, \"Yaw\": 180 } Camera1 is the name of the camera. This name will be used to reference the camera when collecing images. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in ENU frame. Roll , Pitch and Yaw are rotations in degrees. ImageType describes the type of camera. At this moment only rgb and depth cameras are supported. For rgb camera, set this value to 0 and for depth camera set the value to 2. FOV_Degrees describes how much the camera sees . The vertical FoV will be automatically calculated using the following formula: vertical FoV = image height / image width * horizontal FoV . Python \"\"\" Args: requests (list[ImageRequest]): Images required vehicle_name (str, optional): Name of vehicle associated with the camera Returns: list[ImageResponse] \"\"\" [image] = client.simGetImages([fsds.ImageRequest()], vehicle_name = 'FSCar') # For color images: [image] = client.simGetImages([fsds.ImageRequest(camera_name = 'Camera1', image_type = fsds.ImageType.Scene, pixels_as_float = False, compress = False)], vehicle_name = 'FSCar') # For depth images images: [image] = client.simGetImages([fsds.ImageRequest(camera_name = 'Camera1', image_type = fsds.ImageType.DepthPerspective, pixels_as_float = True, compress = False)], vehicle_name = 'FSCar') The API simGetImages can accept request for multiple image types from any cameras in single call. You can specify if image is png compressed, RGB uncompressed or float array. For png compressed images, you get binary string literal. For float array you get Python list of float64. See this file with an example on how get acolor image and write it to a file. Do you want to hep with writing more examples? ROS When using the ROS bridge, images are published on the /fsds/camera/CAMERA_NAME topic, where CAMERA_NAME will be replaced by the name defined in the settings.json . Every camera specified in the settings.json file get's its own topic. The message type is sensor_msgs/Image . The encoding of the image data is bgra8 for color images and 32FC1 for depth images. The ROS bridge regularly publishes static transforms between the fsds/FSCar frame and every camera. Naming of the camera frames is fsds/CAMERA_NAME . For example, the position and orientation of a camera named Test will become available in the frame /fsds/Test .","title":"Camera"},{"location":"camera/#cameras","text":"You know what camera's are. There are two camera types: RGB camera captures color images, just like any normal normal video camera. Depth Cameras (aka DepthPerspective) act as follows: each pixel is given a float value in meters corresponding to the smallest distance from the camera to that point. It captures the world in 3d! At the moment there is no added noise to the images.","title":"Cameras"},{"location":"camera/#add-a-camera-to-the-car","text":"To add a camera to your vehicle, add the following json to the Cameras map in your settings.json : \"Camera1\": { \"CaptureSettings\": [{ \"ImageType\": 0, \"Width\": 785, \"Height\": 785, \"FOV_Degrees\": 90 }], \"X\": 1.0, \"Y\": 0.06, \"Z\": -2.20, \"Pitch\": 0.0, \"Roll\": 0.0, \"Yaw\": 180 } Camera1 is the name of the camera. This name will be used to reference the camera when collecing images. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in ENU frame. Roll , Pitch and Yaw are rotations in degrees. ImageType describes the type of camera. At this moment only rgb and depth cameras are supported. For rgb camera, set this value to 0 and for depth camera set the value to 2. FOV_Degrees describes how much the camera sees . The vertical FoV will be automatically calculated using the following formula: vertical FoV = image height / image width * horizontal FoV .","title":"Add a camera to the car"},{"location":"camera/#python","text":"\"\"\" Args: requests (list[ImageRequest]): Images required vehicle_name (str, optional): Name of vehicle associated with the camera Returns: list[ImageResponse] \"\"\" [image] = client.simGetImages([fsds.ImageRequest()], vehicle_name = 'FSCar') # For color images: [image] = client.simGetImages([fsds.ImageRequest(camera_name = 'Camera1', image_type = fsds.ImageType.Scene, pixels_as_float = False, compress = False)], vehicle_name = 'FSCar') # For depth images images: [image] = client.simGetImages([fsds.ImageRequest(camera_name = 'Camera1', image_type = fsds.ImageType.DepthPerspective, pixels_as_float = True, compress = False)], vehicle_name = 'FSCar') The API simGetImages can accept request for multiple image types from any cameras in single call. You can specify if image is png compressed, RGB uncompressed or float array. For png compressed images, you get binary string literal. For float array you get Python list of float64. See this file with an example on how get acolor image and write it to a file. Do you want to hep with writing more examples?","title":"Python"},{"location":"camera/#ros","text":"When using the ROS bridge, images are published on the /fsds/camera/CAMERA_NAME topic, where CAMERA_NAME will be replaced by the name defined in the settings.json . Every camera specified in the settings.json file get's its own topic. The message type is sensor_msgs/Image . The encoding of the image data is bgra8 for color images and 32FC1 for depth images. The ROS bridge regularly publishes static transforms between the fsds/FSCar frame and every camera. Naming of the camera frames is fsds/CAMERA_NAME . For example, the position and orientation of a camera named Test will become available in the frame /fsds/Test .","title":"ROS"},{"location":"competition-signals/","text":"Competition system flow and Signals Staging At some point, your AS will be staged: The vehicle is placed prior to the starting line, the fsds_ros_bridge node will connect to the ROS system. From this point onwards, the AS will receive sensor data and can control the vehicle by publishing vehicle setpoints. However, it shouldn't start driving the vehicle just yet! Starting Just like with a physical FS event, the vehicle must only start driving after a GO signal is received. The GO signal indicates that the AS must start the mission and that the vehicle should start driving. This signal is also known as the 'green flag' signal or 'starting' signal. Within this repo, we will reference it as 'GO'. Within the GO signal, the mission and track are added. Currently, autocross and trackdrive are the supported missions. In autocross, the vehicle has to complete a single lap on an unknown track. On the trackdrive, the vehicle has to finish 10 laps on a track it has previously seen. During the competition, on each track, every AS will do an autocross mission before a trackdrive mission. It can occur that multiple autocross runs on different tracks take place before going to trackdrive. It can also happen that multiple autocross runs take place on the same track. For example, the AS might be requested to do: autocross on track A autocross on track B trackdrive on track A autocross on track C autocross on track C (re-run) trackdrive on track C The AS must implement the following behaviour: When the AS is requested to do autocross on a track that it has seen before, it must delete any and all data it gathered during all previous runs on this track. When the AS is requested to do trackdrive on a track where it has done a trackdrive previously, it must delete any and all data it gathered during all previous trackdrive runs on this track. However, the data gathered during the last autocross on this track shouldn't be deleted. An exception to this rule is data recorded with the exclusive intent to analyze the AS's behaviour after the event. This includes all files that the AS only writes to but does not read from. To make the AS aware of which track it is driving, the GO signal includes a unique identifier of the upcoming track. After the initial GO signal, the signal is continuously re-sent at 1 Hz to ensure it arrives at the team's AS. The timestamp of all consecutive GO signals is equal to the first one. Finishing There are two ways to conclude a run: finishing or stopping. When the autonomous system feels it concluded it's run, it should send a FINISHED signal. The FINISHED signal tells the simulator that the AS no longer wants to control the vehicle. As such, the simulator will stop the fsds_ros_bridge and the AS will no longer receive sensor data or be able to control the vehicle. When the official decides that the run is over it will stop the simulation. See the rulebook for a description of when the official does so. When the simulation is stopped the fsds_ros_bridge is stopped immediately and the AS will no longer receive sensor data or be able to control the vehicle. The AS will not receive a signal that this happened. To detect a stop, the AS should keep an eye on the GO signal. The general rule is: If the AS did not receive a GO signal for 4 seconds the AS can assume the fsds_ros_bridge is stopped. When this state is detected, the AS can reset itself and prepare for the next mission. Ros messages The AS will receive the GO signal on the following topic: /fsds/signal/go And it AS can publish the FINISHED on this topic: /fsds/signal/finished The AS must publish vehicle control commands on this topic: /fsds/control_command Read more about the techincal detalis of these topics in the ros-bridge documentation","title":"System flow and signals"},{"location":"competition-signals/#competition-system-flow-and-signals","text":"","title":"Competition system flow and Signals"},{"location":"competition-signals/#staging","text":"At some point, your AS will be staged: The vehicle is placed prior to the starting line, the fsds_ros_bridge node will connect to the ROS system. From this point onwards, the AS will receive sensor data and can control the vehicle by publishing vehicle setpoints. However, it shouldn't start driving the vehicle just yet!","title":"Staging"},{"location":"competition-signals/#starting","text":"Just like with a physical FS event, the vehicle must only start driving after a GO signal is received. The GO signal indicates that the AS must start the mission and that the vehicle should start driving. This signal is also known as the 'green flag' signal or 'starting' signal. Within this repo, we will reference it as 'GO'. Within the GO signal, the mission and track are added. Currently, autocross and trackdrive are the supported missions. In autocross, the vehicle has to complete a single lap on an unknown track. On the trackdrive, the vehicle has to finish 10 laps on a track it has previously seen. During the competition, on each track, every AS will do an autocross mission before a trackdrive mission. It can occur that multiple autocross runs on different tracks take place before going to trackdrive. It can also happen that multiple autocross runs take place on the same track. For example, the AS might be requested to do: autocross on track A autocross on track B trackdrive on track A autocross on track C autocross on track C (re-run) trackdrive on track C The AS must implement the following behaviour: When the AS is requested to do autocross on a track that it has seen before, it must delete any and all data it gathered during all previous runs on this track. When the AS is requested to do trackdrive on a track where it has done a trackdrive previously, it must delete any and all data it gathered during all previous trackdrive runs on this track. However, the data gathered during the last autocross on this track shouldn't be deleted. An exception to this rule is data recorded with the exclusive intent to analyze the AS's behaviour after the event. This includes all files that the AS only writes to but does not read from. To make the AS aware of which track it is driving, the GO signal includes a unique identifier of the upcoming track. After the initial GO signal, the signal is continuously re-sent at 1 Hz to ensure it arrives at the team's AS. The timestamp of all consecutive GO signals is equal to the first one.","title":"Starting"},{"location":"competition-signals/#finishing","text":"There are two ways to conclude a run: finishing or stopping. When the autonomous system feels it concluded it's run, it should send a FINISHED signal. The FINISHED signal tells the simulator that the AS no longer wants to control the vehicle. As such, the simulator will stop the fsds_ros_bridge and the AS will no longer receive sensor data or be able to control the vehicle. When the official decides that the run is over it will stop the simulation. See the rulebook for a description of when the official does so. When the simulation is stopped the fsds_ros_bridge is stopped immediately and the AS will no longer receive sensor data or be able to control the vehicle. The AS will not receive a signal that this happened. To detect a stop, the AS should keep an eye on the GO signal. The general rule is: If the AS did not receive a GO signal for 4 seconds the AS can assume the fsds_ros_bridge is stopped. When this state is detected, the AS can reset itself and prepare for the next mission.","title":"Finishing"},{"location":"competition-signals/#ros-messages","text":"The AS will receive the GO signal on the following topic: /fsds/signal/go And it AS can publish the FINISHED on this topic: /fsds/signal/finished The AS must publish vehicle control commands on this topic: /fsds/control_command Read more about the techincal detalis of these topics in the ros-bridge documentation","title":"Ros messages"},{"location":"coordinate-frames/","text":"Coordinate frames & rotation The simulator coordinates in the ENU coordinate system (x = east, y = north, z = up) where x is pointing forward . All rotations are right-handed . Yaw is zero when pointing forward/east. If possible, we stick to the above descirbed coordinate system. This system was chosen as it seems to be the the default for (autonomous) driving ( 1 , 2 ) and it is (the default for ROS)[https://www.ros.org/reps/rep-0103.html]. All data in global frame, that is all things not relative to the vehicle like position of the car odometry or cone positions, are relative to the starting position of the car. For example, if the car spawned facing east and moved forward 1 meter and 0.5 meter right, it ends up at global position X=1;Y=-0.5;Z=0. Unreal Engine This information is only relevant for developers of this project Unreal Engine uses a different coordinate system: x = forward = north y = right = east z = up roll = right handed around the x axis pitch = right handed around the y axis yaw = left handed around the z axis Translations between UU and ENU are coded in CoordFrameTransformer.h .","title":"Coordinate frames"},{"location":"coordinate-frames/#coordinate-frames-rotation","text":"The simulator coordinates in the ENU coordinate system (x = east, y = north, z = up) where x is pointing forward . All rotations are right-handed . Yaw is zero when pointing forward/east. If possible, we stick to the above descirbed coordinate system. This system was chosen as it seems to be the the default for (autonomous) driving ( 1 , 2 ) and it is (the default for ROS)[https://www.ros.org/reps/rep-0103.html]. All data in global frame, that is all things not relative to the vehicle like position of the car odometry or cone positions, are relative to the starting position of the car. For example, if the car spawned facing east and moved forward 1 meter and 0.5 meter right, it ends up at global position X=1;Y=-0.5;Z=0.","title":"Coordinate frames &amp; rotation"},{"location":"coordinate-frames/#unreal-engine","text":"This information is only relevant for developers of this project Unreal Engine uses a different coordinate system: x = forward = north y = right = east z = up roll = right handed around the x axis pitch = right handed around the y axis yaw = left handed around the z axis Translations between UU and ENU are coded in CoordFrameTransformer.h .","title":"Unreal Engine"},{"location":"docker_ubuntu/","text":"FSDS Simulator on Docker in Linux This guide describes how to build a Docker image from FSDS linux binaries . You could instead compile Unreal Engine + AirSim from source, but this is not documented. This Docker implementation is based on Micorsoft's Docker implementation for AIRSIM repository. Binaries Requirements: Install nvidia-docker2 Install Ros-melodic Get FSDS Repo same version as binary used Ros-Bridge Build the Docker image Use the build_airsim_image.py to build the docker image NVIDIA vulkan is thebase image on which FSDS is installed. By default Ubuntu 18.04 with CUDA 10.0 is used. You can specify any NVIDIA vulkan at your own risk. Use --target_image is the desired name of your Docker image. It defaults to fsdsairsim_binary with tag as vulkan-ubuntu18.04 To customize the version, use: $ cd docker/src; $ python build_airsim_image.py --target_image=fsdsairsim_binary:vulkan-ubuntu18.04 After building the image, verify that the image exists by runnig: $ docker images | grep fsdsairsim_binary Run FSDS inside the Docker container Get the binary , or package your own project in Ubuntu. You can either download the latest version yourself, or use the download_FSDSSimulator_binary.sh helper script. Now, run fsds inside the Docker container: $ ./run_airsim_image_binary.sh DOCKER_IMAGE_NAME UNREAL_BINARY_SHELL_SCRIPT UNREAL_BINARY_ARGUMENTS -- headless Replace the variables as follows: * DOCKER_IMAGE_NAME : Same as target_image parameter in previous step. By default, enter fsdsairsim_binary:vulkan-ubuntu18.04 * UNREAL_BINARY_SHELL_SCRIPT : for FSDSsimulator enviroment, it will be fsds-v2.1.0-linux/FSDS.sh * UNREAL_BINARY_ARGUMENTS : For FSDSsimulator, most relevant would be -windowed , -ResX , -ResY . See here all options . For FSDSsimulator, you can do a $ ./run_airsim_image_binary.sh fsdsairsim_binary:vulkan-ubuntu18.04 fsds-v2.1.0-linux/FSDS.sh -windowed -ResX=1080 -ResY=720 To run in headless mode, use suffix -- headless at the end: $ ./run_airsim_image_binary.sh fsds-v2.1.0-linux/FSDS.sh -- headless You need to have a settings.json file in the current working directory from where you run ./run_airsim_image_binary.sh .","title":"Docker"},{"location":"docker_ubuntu/#fsds-simulator-on-docker-in-linux","text":"This guide describes how to build a Docker image from FSDS linux binaries . You could instead compile Unreal Engine + AirSim from source, but this is not documented. This Docker implementation is based on Micorsoft's Docker implementation for AIRSIM repository.","title":"FSDS Simulator on Docker in Linux"},{"location":"docker_ubuntu/#binaries","text":"","title":"Binaries"},{"location":"docker_ubuntu/#requirements","text":"Install nvidia-docker2 Install Ros-melodic Get FSDS Repo same version as binary used Ros-Bridge","title":"Requirements:"},{"location":"docker_ubuntu/#build-the-docker-image","text":"Use the build_airsim_image.py to build the docker image NVIDIA vulkan is thebase image on which FSDS is installed. By default Ubuntu 18.04 with CUDA 10.0 is used. You can specify any NVIDIA vulkan at your own risk. Use --target_image is the desired name of your Docker image. It defaults to fsdsairsim_binary with tag as vulkan-ubuntu18.04 To customize the version, use: $ cd docker/src; $ python build_airsim_image.py --target_image=fsdsairsim_binary:vulkan-ubuntu18.04 After building the image, verify that the image exists by runnig: $ docker images | grep fsdsairsim_binary","title":"Build the Docker image"},{"location":"docker_ubuntu/#run-fsds-inside-the-docker-container","text":"Get the binary , or package your own project in Ubuntu. You can either download the latest version yourself, or use the download_FSDSSimulator_binary.sh helper script. Now, run fsds inside the Docker container: $ ./run_airsim_image_binary.sh DOCKER_IMAGE_NAME UNREAL_BINARY_SHELL_SCRIPT UNREAL_BINARY_ARGUMENTS -- headless Replace the variables as follows: * DOCKER_IMAGE_NAME : Same as target_image parameter in previous step. By default, enter fsdsairsim_binary:vulkan-ubuntu18.04 * UNREAL_BINARY_SHELL_SCRIPT : for FSDSsimulator enviroment, it will be fsds-v2.1.0-linux/FSDS.sh * UNREAL_BINARY_ARGUMENTS : For FSDSsimulator, most relevant would be -windowed , -ResX , -ResY . See here all options . For FSDSsimulator, you can do a $ ./run_airsim_image_binary.sh fsdsairsim_binary:vulkan-ubuntu18.04 fsds-v2.1.0-linux/FSDS.sh -windowed -ResX=1080 -ResY=720 To run in headless mode, use suffix -- headless at the end: $ ./run_airsim_image_binary.sh fsds-v2.1.0-linux/FSDS.sh -- headless You need to have a settings.json file in the current working directory from where you run ./run_airsim_image_binary.sh .","title":"Run FSDS inside the Docker container"},{"location":"gcp-remote-workstation/","text":"How to deploy a remote workstation on Google Cloud Platform In this tutorial we create an gcp instance that is configured for running and developing the Formula-Student-Driverless-Simulator. 1. Create a new instance and configure ports We assume you are familiar with google cloud configurations. Create a new instance in the region of your choice. Minimum requirements are: 12 vCPU 24GB memory Any NVIDIA GPU 150GB Disk Recommended specs are: 16 vCPU 32GB memory 1 NVIDIA Tesla P100 300GB SSD Disk CPU platform Sandy Bridge Do not enable NVIDIA GRID Choose Windows Server 2019 Datacenter Check 'Attach display device' and 'enable NVidia Grid' Ensure this instance has a public ip. 2. Access the remote desktop Go to the cloud instance and use the 'Set Windows password' to set a password for your user. From Ubuntu Install a remote desktop client: sudo apt-get install remmina Launch Remmina, add a new connection. Set Server to the ip of the instance username to the name you entered when setting the password password to the password you created before Color depth to `High color (16 bpp) From Windows Click the little arrow next to the 'RDP' button and click 'Download the RDP file'. Double click on the downloaded file. Use the credentials you just created to login. 3. Disable internet security We need to disable internet security protection because we want to download a bunch of tools. Start the Server Manager. Select Local Server (The server you are currently on and the one that needs IE Enhanced Security disabled) On the right side of the Server Manager, find the IE Enhanced Security Configuration Setting. Disable it. Open Internet Options, go to tab 'Security' set the security level for 'internet' to 'Medium'. Disable 'Protection Mode'. Now you can use internet explorer to downlaod firefox. 4. Install NVIDIA drivers Follow this tutorial to install the required nvidia drivers. Restart your computer. Validate the installation by running the following command in a powershell terminal & 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe' 5. Install .NET and Windows Subsystem for Linux Start the Server Manager. Click 'Manage', 'Add Roles and Features' Click Next until you find 'Server Roles'. In 'Server Roles', select 'Remote Desktop Services' In 'Features', select '.NET Framework 3.5' and 'Windows Subsystem for Linux'. Click install. You can ignore the warning about missing source files. Restart the computer","title":"Google cloud remote workstation"},{"location":"gcp-remote-workstation/#how-to-deploy-a-remote-workstation-on-google-cloud-platform","text":"In this tutorial we create an gcp instance that is configured for running and developing the Formula-Student-Driverless-Simulator.","title":"How to deploy a remote workstation on Google Cloud Platform"},{"location":"gcp-remote-workstation/#1-create-a-new-instance-and-configure-ports","text":"We assume you are familiar with google cloud configurations. Create a new instance in the region of your choice. Minimum requirements are: 12 vCPU 24GB memory Any NVIDIA GPU 150GB Disk Recommended specs are: 16 vCPU 32GB memory 1 NVIDIA Tesla P100 300GB SSD Disk CPU platform Sandy Bridge Do not enable NVIDIA GRID Choose Windows Server 2019 Datacenter Check 'Attach display device' and 'enable NVidia Grid' Ensure this instance has a public ip.","title":"1. Create a new instance and configure ports"},{"location":"gcp-remote-workstation/#2-access-the-remote-desktop","text":"Go to the cloud instance and use the 'Set Windows password' to set a password for your user.","title":"2. Access the remote desktop"},{"location":"gcp-remote-workstation/#from-ubuntu","text":"Install a remote desktop client: sudo apt-get install remmina Launch Remmina, add a new connection. Set Server to the ip of the instance username to the name you entered when setting the password password to the password you created before Color depth to `High color (16 bpp)","title":"From Ubuntu"},{"location":"gcp-remote-workstation/#from-windows","text":"Click the little arrow next to the 'RDP' button and click 'Download the RDP file'. Double click on the downloaded file. Use the credentials you just created to login.","title":"From Windows"},{"location":"gcp-remote-workstation/#3-disable-internet-security","text":"We need to disable internet security protection because we want to download a bunch of tools. Start the Server Manager. Select Local Server (The server you are currently on and the one that needs IE Enhanced Security disabled) On the right side of the Server Manager, find the IE Enhanced Security Configuration Setting. Disable it. Open Internet Options, go to tab 'Security' set the security level for 'internet' to 'Medium'. Disable 'Protection Mode'. Now you can use internet explorer to downlaod firefox.","title":"3. Disable internet security"},{"location":"gcp-remote-workstation/#4-install-nvidia-drivers","text":"Follow this tutorial to install the required nvidia drivers. Restart your computer. Validate the installation by running the following command in a powershell terminal & 'C:\\Program Files\\NVIDIA Corporation\\NVSMI\\nvidia-smi.exe'","title":"4. Install NVIDIA drivers"},{"location":"gcp-remote-workstation/#5-install-net-and-windows-subsystem-for-linux","text":"Start the Server Manager. Click 'Manage', 'Add Roles and Features' Click Next until you find 'Server Roles'. In 'Server Roles', select 'Remote Desktop Services' In 'Features', select '.NET Framework 3.5' and 'Windows Subsystem for Linux'. Click install. You can ignore the warning about missing source files. Restart the computer","title":"5. Install .NET and Windows Subsystem for Linux"},{"location":"getting-started-with-python/","text":"Connecting to the simulator with Python You can use the python client to connect the simulator. The python client is able to retrieve sensordata and send vehicle controll. Dependencies The Python client depends on msgpack, numpy and opencv-contrib. Install the dependencies like this: pip install -r requirements.txt Getting started Let's drive the car forward! # This code adds the fsds package to the pyhthon path. # It assumes the fsds repo is cloned in the home directory. # Replace fsds_lib_path with a path to wherever the python directory is located. import sys, os fsds_lib_path = os.path.join(os.path.expanduser(\"~\"), \"Formula-Student-Driverless-Simulator\", \"python\") sys.path.insert(0, fsds_lib_path) import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() # After enabling api controll only the api can controll the car. # Direct keyboard and joystick into the simulator are disabled. # If you want to still be able to drive with the keyboard while also # controll the car using the api, call client.enableApiControl(False) client.enableApiControl(True) # Instruct the car to go full-speed forward car_controls = fsds.CarControls() car_controls.throttle = 1 client.setCarControls(car_controls) time.sleep(5) # Places the vehicle back at it's original position client.reset() A full example of an autonomous system that can finish a lap can be found here Find more examples here. Sensors Documentation on requesting and processing sensordata can be found in the respective sensor documentation pages: Lidar Camera GPS IMU Ground Speed Sensor Getting ground truth information Using the following function you get get the real, latest position of the car: state = client.getCarState() # velocity in m/s in the car's reference frame print(state.speed) # nanosecond timestamp of the latest physics update print(state.timestamp) # position (meter) in global reference frame. print(state.kinematics_estimated.position) # orientation (Quaternionr) in global reference frame. print(state.kinematics_estimated.orientation) # m/s print(state.kinematics_estimated.linear_velocity) # rad/s print(state.kinematics_estimated.angular_velocity) # m/s^2 print(state.kinematics_estimated.linear_acceleration) # rad/s^2 print(state.kinematics_estimated.angular_acceleration)","title":"Getting started with Python"},{"location":"getting-started-with-python/#connecting-to-the-simulator-with-python","text":"You can use the python client to connect the simulator. The python client is able to retrieve sensordata and send vehicle controll.","title":"Connecting to the simulator with Python"},{"location":"getting-started-with-python/#dependencies","text":"The Python client depends on msgpack, numpy and opencv-contrib. Install the dependencies like this: pip install -r requirements.txt","title":"Dependencies"},{"location":"getting-started-with-python/#getting-started","text":"Let's drive the car forward! # This code adds the fsds package to the pyhthon path. # It assumes the fsds repo is cloned in the home directory. # Replace fsds_lib_path with a path to wherever the python directory is located. import sys, os fsds_lib_path = os.path.join(os.path.expanduser(\"~\"), \"Formula-Student-Driverless-Simulator\", \"python\") sys.path.insert(0, fsds_lib_path) import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() # After enabling api controll only the api can controll the car. # Direct keyboard and joystick into the simulator are disabled. # If you want to still be able to drive with the keyboard while also # controll the car using the api, call client.enableApiControl(False) client.enableApiControl(True) # Instruct the car to go full-speed forward car_controls = fsds.CarControls() car_controls.throttle = 1 client.setCarControls(car_controls) time.sleep(5) # Places the vehicle back at it's original position client.reset() A full example of an autonomous system that can finish a lap can be found here Find more examples here.","title":"Getting started"},{"location":"getting-started-with-python/#sensors","text":"Documentation on requesting and processing sensordata can be found in the respective sensor documentation pages: Lidar Camera GPS IMU Ground Speed Sensor","title":"Sensors"},{"location":"getting-started-with-python/#getting-ground-truth-information","text":"Using the following function you get get the real, latest position of the car: state = client.getCarState() # velocity in m/s in the car's reference frame print(state.speed) # nanosecond timestamp of the latest physics update print(state.timestamp) # position (meter) in global reference frame. print(state.kinematics_estimated.position) # orientation (Quaternionr) in global reference frame. print(state.kinematics_estimated.orientation) # m/s print(state.kinematics_estimated.linear_velocity) # rad/s print(state.kinematics_estimated.angular_velocity) # m/s^2 print(state.kinematics_estimated.linear_acceleration) # rad/s^2 print(state.kinematics_estimated.angular_acceleration)","title":"Getting ground truth information"},{"location":"getting-started-with-ros/","text":"Connecting to the simulator with ROS You can use the ROS bridge to connect the simulator to ROS1 and ROS2. The ROS bridge will publish the sensordata from the simulator into ROS topics. Your autonomous system will be able to publish car-control messages which the ROS bridge will send to the simulator. The ROS bridge works on Ubuntu or in WSL in windows (but it is harder to set up). If you have the simulator running on windows we reccommend Windows Subsystem for Linux. This offers a virtual Ubuntu machine within Windows. You can read here how to install it . While you are at it you might also want to install Xming so you can run rviz and rqt_plot from within WSL. Requirements The ROS bridge requires ROS Melodic/Noetic/Galactic to be installed , as well as the following dependencies: sudo apt-get install ros-melodic-tf2-geometry-msgs python-catkin-tools ros-melodic-rqt-multiplot ros-melodic-joy ros-melodic-cv-bridge ros-melodic-image-transport libyaml-cpp-dev libcurl4-openssl-dev Cloning the repository Before you clone, make sure you have git lfs installed! Ready? Lets clone the repo into your home directory: git clone git@github.com:FS-Driverless/Formula-Student-Driverless-Simulator.git --recurse-submodules If you haven't setup your ssh keys, you can clone using https by running the following command: git clone https://github.com/FS-Driverless/Formula-Student-Driverless-Simulator.git --recurse-submodules THE REPO HAS TO BE CLONED IN THE HOME DIRECTORY! . So the repo location should be $HOME/Formula-Student-Driverless-Simulator . Why you ask? Because we couldn't get relative paths in the C++ code to work so now we have hard-coded some paths to the home directory. I know yes it is ugly but it works. If you are bothered by it I would welcome you to open a pr with a fix. If this folder already exists as a result of any previous step, move the existing folder out of the way and merge the content afterwards. If you are on Windows and cloned this repository in a Windows directory, go into the cloned repo and run git config core.fileMode false to ignore file mode changes. If you want to share the the cloned directory with the Ubuntu WSL system, create a symlink within WSL like so: ln -s /mnt/c/Users/developer/Formula-Student-Driverless-Simulator ~/Formula-Student-Driverless-Simulator Now, checkout the version equal to the simulator. If you are running for example simulator packaged version v2.1.0, run git checkout tags/v2.1.0 to get the ROS brige to the same version Preparing AirLib AirLib is the shared code between the ROS wrapper and the AirSim Unreal Engine plugin. We need to stage the source before we can compile it together with the wrapper. If you are working in a WSL shared folder where previously build.cmd was ran, you can skip this step. Open an Ubuntu terminal and run AirSim/setup.sh . This will download the nessesary libraries required to compile AirLib. You will only need to run this once. Everything setup.sh does is also included in build.cmd. Building the workspace cd ros catkin init catkin config --cmake-args -DCMAKE_BUILD_TYPE=Release #(Optional) catkin build Launching the ros bridge The ROS bridge consists of a few nodes to achieve the highest performance and keep the codebase clean. Everything can be launched using the fsds_ros_bridge.launch launchfile. cd ros source devel/setup.bash roslaunch fsds_ros_bridge fsds_ros_bridge.launch The ROS bridge will read the settings from ~/Formula-Student-Driverless-Simulator/settings.json . Make sure this is the same configuration file as the simulator uses. Read all about configuring the ROS bridge here. Connecting your autonomous system The ROS bridge of this simulator had to make use of several custom msgs (for control commands, the groundtruth track, etc). These messages are defined in a ROS package called fs_msgs which is located in a separate, light repository . To implement publishers and subscibers for these messages types in your autonomous pipeline, you will have to add the fs_msgs repository as a submodule in your codebase (inside de src directory of an existing catkin workspace as done in this repository) or clone it and build it somewhere else in your system. Now, all that is left to do is subscribe to the following topics to receive sensordata /fsds/gps /fsds/imu /fsds/camera/CAMERA_NAME /fsds/camera/CAMERA_NAME/camera_info /fsds/lidar/LIDAR_NAME /fsds/testing_only/odom /fsds/testing_only/track /fsds/testing_only/extra_info and publish to the following topic /fsds/control_command to publish the vehicle control setpoints. Multiple computers If you have 2 computer, you can run the simulator and your autonomous system each on their own computer. But where does the ROS-bridge run? You have 2 options: Run the ROS bridge on the same computer as your autonomous system. The ROS bridge will connect to the simulator using a TCP connection to send control commands and receive sensor data. The ROS bridge will use local ROS topics to communicate with the autonomous system. Use the host argument in the fsds_ros_bridge.launch file to tell the ROS bridge where the simulator is at. Ensure firewall rules allow the ROS bridge to connect to the simulator on port 41451. Run the ROS bridge on the same computer as the simulator. Your autonomous system would use ROS multi-computer networking to publish/subscribe to FSDS topics. Follow this tutorial and this one on the ROS Wiki to learn how to do this. If you have never worked with a multi-computer ROS networking before, option 1 is probably the way to go. If you are running the simulator on Windows, option 1 is the easiest as well. You can run the ROS bridge within WSL and use option 2 but there are some constraints, see below. Notes on running the ROS bridge in WSL. It is possible to run the ROS bridge in Windows Subsystem Linux (WSL). However, when using WSL with a multi-computer ROS setup, things get weird . The problem is that everything crashes when you run the ROS bridge in WSL and try to send control commands via ROS from a different computer on the network. To work around this problem, you should know: The only way to reliably have a ROS bridge in WSL receive control commands from another computer is to send these messages using UDP from a C++ node. To enable UDP on the control setpoint topic, set the UDP_control argument like so: roslaunch fsds_ros_bridge fsds_ros_bridge.launch UDP_control:=true UDP is only supported in roscpp. If you are using a Python node to send controll commands, UDP won't help a thing. This setup has been tested on WSL 1 (Ubuntu 18.04), second machine (Ubuntu 18.04) with ROS melodic on both machines. If you are using WSL 2 and you manage to get ROS to work with 2 machines, please help us understand how by writing a comment on this issue.","title":"Getting started with ROS"},{"location":"getting-started-with-ros/#connecting-to-the-simulator-with-ros","text":"You can use the ROS bridge to connect the simulator to ROS1 and ROS2. The ROS bridge will publish the sensordata from the simulator into ROS topics. Your autonomous system will be able to publish car-control messages which the ROS bridge will send to the simulator. The ROS bridge works on Ubuntu or in WSL in windows (but it is harder to set up). If you have the simulator running on windows we reccommend Windows Subsystem for Linux. This offers a virtual Ubuntu machine within Windows. You can read here how to install it . While you are at it you might also want to install Xming so you can run rviz and rqt_plot from within WSL.","title":"Connecting to the simulator with ROS"},{"location":"getting-started-with-ros/#requirements","text":"The ROS bridge requires ROS Melodic/Noetic/Galactic to be installed , as well as the following dependencies: sudo apt-get install ros-melodic-tf2-geometry-msgs python-catkin-tools ros-melodic-rqt-multiplot ros-melodic-joy ros-melodic-cv-bridge ros-melodic-image-transport libyaml-cpp-dev libcurl4-openssl-dev","title":"Requirements"},{"location":"getting-started-with-ros/#cloning-the-repository","text":"Before you clone, make sure you have git lfs installed! Ready? Lets clone the repo into your home directory: git clone git@github.com:FS-Driverless/Formula-Student-Driverless-Simulator.git --recurse-submodules If you haven't setup your ssh keys, you can clone using https by running the following command: git clone https://github.com/FS-Driverless/Formula-Student-Driverless-Simulator.git --recurse-submodules THE REPO HAS TO BE CLONED IN THE HOME DIRECTORY! . So the repo location should be $HOME/Formula-Student-Driverless-Simulator . Why you ask? Because we couldn't get relative paths in the C++ code to work so now we have hard-coded some paths to the home directory. I know yes it is ugly but it works. If you are bothered by it I would welcome you to open a pr with a fix. If this folder already exists as a result of any previous step, move the existing folder out of the way and merge the content afterwards. If you are on Windows and cloned this repository in a Windows directory, go into the cloned repo and run git config core.fileMode false to ignore file mode changes. If you want to share the the cloned directory with the Ubuntu WSL system, create a symlink within WSL like so: ln -s /mnt/c/Users/developer/Formula-Student-Driverless-Simulator ~/Formula-Student-Driverless-Simulator Now, checkout the version equal to the simulator. If you are running for example simulator packaged version v2.1.0, run git checkout tags/v2.1.0 to get the ROS brige to the same version","title":"Cloning the repository"},{"location":"getting-started-with-ros/#preparing-airlib","text":"AirLib is the shared code between the ROS wrapper and the AirSim Unreal Engine plugin. We need to stage the source before we can compile it together with the wrapper. If you are working in a WSL shared folder where previously build.cmd was ran, you can skip this step. Open an Ubuntu terminal and run AirSim/setup.sh . This will download the nessesary libraries required to compile AirLib. You will only need to run this once. Everything setup.sh does is also included in build.cmd.","title":"Preparing AirLib"},{"location":"getting-started-with-ros/#building-the-workspace","text":"cd ros catkin init catkin config --cmake-args -DCMAKE_BUILD_TYPE=Release #(Optional) catkin build","title":"Building the workspace"},{"location":"getting-started-with-ros/#launching-the-ros-bridge","text":"The ROS bridge consists of a few nodes to achieve the highest performance and keep the codebase clean. Everything can be launched using the fsds_ros_bridge.launch launchfile. cd ros source devel/setup.bash roslaunch fsds_ros_bridge fsds_ros_bridge.launch The ROS bridge will read the settings from ~/Formula-Student-Driverless-Simulator/settings.json . Make sure this is the same configuration file as the simulator uses. Read all about configuring the ROS bridge here.","title":"Launching the ros bridge"},{"location":"getting-started-with-ros/#connecting-your-autonomous-system","text":"The ROS bridge of this simulator had to make use of several custom msgs (for control commands, the groundtruth track, etc). These messages are defined in a ROS package called fs_msgs which is located in a separate, light repository . To implement publishers and subscibers for these messages types in your autonomous pipeline, you will have to add the fs_msgs repository as a submodule in your codebase (inside de src directory of an existing catkin workspace as done in this repository) or clone it and build it somewhere else in your system. Now, all that is left to do is subscribe to the following topics to receive sensordata /fsds/gps /fsds/imu /fsds/camera/CAMERA_NAME /fsds/camera/CAMERA_NAME/camera_info /fsds/lidar/LIDAR_NAME /fsds/testing_only/odom /fsds/testing_only/track /fsds/testing_only/extra_info and publish to the following topic /fsds/control_command to publish the vehicle control setpoints.","title":"Connecting your autonomous system"},{"location":"getting-started-with-ros/#multiple-computers","text":"If you have 2 computer, you can run the simulator and your autonomous system each on their own computer. But where does the ROS-bridge run? You have 2 options: Run the ROS bridge on the same computer as your autonomous system. The ROS bridge will connect to the simulator using a TCP connection to send control commands and receive sensor data. The ROS bridge will use local ROS topics to communicate with the autonomous system. Use the host argument in the fsds_ros_bridge.launch file to tell the ROS bridge where the simulator is at. Ensure firewall rules allow the ROS bridge to connect to the simulator on port 41451. Run the ROS bridge on the same computer as the simulator. Your autonomous system would use ROS multi-computer networking to publish/subscribe to FSDS topics. Follow this tutorial and this one on the ROS Wiki to learn how to do this. If you have never worked with a multi-computer ROS networking before, option 1 is probably the way to go. If you are running the simulator on Windows, option 1 is the easiest as well. You can run the ROS bridge within WSL and use option 2 but there are some constraints, see below.","title":"Multiple computers"},{"location":"getting-started-with-ros/#notes-on-running-the-ros-bridge-in-wsl","text":"It is possible to run the ROS bridge in Windows Subsystem Linux (WSL). However, when using WSL with a multi-computer ROS setup, things get weird . The problem is that everything crashes when you run the ROS bridge in WSL and try to send control commands via ROS from a different computer on the network. To work around this problem, you should know: The only way to reliably have a ROS bridge in WSL receive control commands from another computer is to send these messages using UDP from a C++ node. To enable UDP on the control setpoint topic, set the UDP_control argument like so: roslaunch fsds_ros_bridge fsds_ros_bridge.launch UDP_control:=true UDP is only supported in roscpp. If you are using a Python node to send controll commands, UDP won't help a thing. This setup has been tested on WSL 1 (Ubuntu 18.04), second machine (Ubuntu 18.04) with ROS melodic on both machines. If you are using WSL 2 and you manage to get ROS to work with 2 machines, please help us understand how by writing a comment on this issue.","title":"Notes on running the ROS bridge in WSL."},{"location":"getting-started/","text":"Getting started When running this simulator there are two steps: running the simulator and connecting your autonomous system. This page helps you run the simulation. After you have finished this page you can go on and connect your autonomous system. To run the simulation smoothly you need quite a fast Windows computer with a modern videocard. We highly recommend the following computer specs. You might be able to run with less power but everything will be slower. For developing this project, you need quite a good computer because Unreal Engine is a heavy baby. 8 core 2.3Ghz CPU 12 GB memory 30GB free SSD storage (120GB when building the unreal project from source) Recent NVidia card with Vulkan support and 3 GB of memory. (You can check the video card drivers by running vulkaninfo ). Different brand video cards might work but have not been tested. If your computer does not suffice you can use a remote workstation on Google Cloud Platform. Read this tutorial on how to setup your virtual workstation. The simulator will load settings from the file Formula-Student-Driverless-Simulator/settings.json in your home directory . This file is required for the simulator to work and contains the sensor configuration of the car. If you clone the repo you will already have this file in place. If not, copy-paste the contents of the settings.json file at the root of this repository into the ~/Formula-Student-Driverless-Simulator . This should get you started with the default sensor configuration, feel free to try your own custom sensor suite. Installation From release binaries Pre-compiled binaries are available for every release. Go to releases and download the latest one. Unzip it to anywhere on your computer and launch FSDS.exe. A window with a car should popup! Try driving the car around using the arrowkeys. If you get a black screen with some buttons, make sure the folder with the binary is in your user folder (Windows: C:\\Users\\username\\Formula-Student-Driverless-Simulator , Linux: ~/Formula-Student-Driverless-Simulator ) If all that works, you can continue to the ROS interface or the python interface . From source using the Unreal Engine Editor Instead of running the simulator from release binaries, you can compile it manually using unreal engine. This is usefull if you want to get the latest changes or if you want to make changes to the maps or the simulation itself. If you want to run the unreal engine project from source you will need unreal engine and visual studio 2019 . On Ubuntu you can skip the visual studio 2019 part, but you still need Unreal Engine. 1. Get the repository You can either download the repo using the big green download button on the github page of this project or clone the repository. For cloning, checkout the documentation on this further down this page. Make sure you clone the repository in your home directory . When downloading or cloning, by default you get the latest, unreleased version. This is probably not the version that you want. Make sure you select the version that you need! 2. Compiling the AirSim plugin The Unreal Engine project requires the AirSim plugin. We have to compile this plugin first. The AirSim plugin is made up of AirLib (/AirSim/AirLib) and the plugin code (/UE4Project/Plugins/AirSim/Source). AirLib is the shared code between the ros wrapper and the AirSim Unreal Engine plugin. First build the AirLib code. On Windows, open the Developer Command Prompt for VS 2019 , go to Formula-Student-Driverless-Simulator/AirSim and run build.cmd On Ubuntu, go to folder AirSim and run setup.sh and build.sh . So what does build.cmd or setup.sh+build.sh do? It downloads any nessesary libraries and compiles AirLib. After compilation it places the files in the UE4Project so that these files can be used durcing compilation of the plugin. The first time this takes quite a while. Go walk around a bit, maybe start playing factoryidle . 3. Working with the Unreal Engine project Launch Unreal Engine, press Browse and open the FSDS project in ~/Driverless-Competition-Simulator/UE4Project/FSOnline.uproject . The project should now open correctly. If it does not, make sure of the following: you have cloned the repository inside your home folder (~/) you have cloned with LFS enabed. If not, run git lfs install and git lfs pull to download the large files. within ~/Driverless-Competition-Simulator/AirSim/ , you have run build.cmd on Windows and ./setup.sh && ./build.sh on Ubuntu. On Ubuntu, we recommend adding the following alias to your ~/.bashrc to speed up the process of opening the repository in the future: alias ue='~/UnrealEngine/Engine/Binaries/Linux/UE4Editor ~/Formula-Student-Driverless-Simulator/UE4Project/FSOnline.uproject' It might show an error like 'This project was made with a different version of the Unreal Engine'. In that case select more options and skip conversion . When asked to rebuild the 'Blocks' and 'AirSim' modules, choose 'Yes'. This is the step where the plugin part of AirSim is compiled. After some time Unreal Engine will start and you can launch the game. Run the game in standalone mode or or selected viewport mode, simulate and eject mode do not support camera's. If you make changes to AirLib you have to run build.cmd again. If you make changes to the plugin code or AirLib, you only have to recompile the plugin. This can be done from within the Unreal Editor. go to to Window -> Developer tools -> Modules . Search for AirSim and click Recompile . 4. Launching the game To run the game, click the big Play button. If you want to run it like it would run when packaged, choose 'Run as standalone game'. Next steps Now you are ready to connect your autonomous system. At the moment there are two integration methods: Use the ROS bridge to connect your ROS autonomous system to the bridge. Use the Python client to interact with the simulator.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"When running this simulator there are two steps: running the simulator and connecting your autonomous system. This page helps you run the simulation. After you have finished this page you can go on and connect your autonomous system. To run the simulation smoothly you need quite a fast Windows computer with a modern videocard. We highly recommend the following computer specs. You might be able to run with less power but everything will be slower. For developing this project, you need quite a good computer because Unreal Engine is a heavy baby. 8 core 2.3Ghz CPU 12 GB memory 30GB free SSD storage (120GB when building the unreal project from source) Recent NVidia card with Vulkan support and 3 GB of memory. (You can check the video card drivers by running vulkaninfo ). Different brand video cards might work but have not been tested. If your computer does not suffice you can use a remote workstation on Google Cloud Platform. Read this tutorial on how to setup your virtual workstation. The simulator will load settings from the file Formula-Student-Driverless-Simulator/settings.json in your home directory . This file is required for the simulator to work and contains the sensor configuration of the car. If you clone the repo you will already have this file in place. If not, copy-paste the contents of the settings.json file at the root of this repository into the ~/Formula-Student-Driverless-Simulator . This should get you started with the default sensor configuration, feel free to try your own custom sensor suite.","title":"Getting started"},{"location":"getting-started/#installation","text":"","title":"Installation"},{"location":"getting-started/#from-release-binaries","text":"Pre-compiled binaries are available for every release. Go to releases and download the latest one. Unzip it to anywhere on your computer and launch FSDS.exe. A window with a car should popup! Try driving the car around using the arrowkeys. If you get a black screen with some buttons, make sure the folder with the binary is in your user folder (Windows: C:\\Users\\username\\Formula-Student-Driverless-Simulator , Linux: ~/Formula-Student-Driverless-Simulator ) If all that works, you can continue to the ROS interface or the python interface .","title":"From release binaries"},{"location":"getting-started/#from-source-using-the-unreal-engine-editor","text":"Instead of running the simulator from release binaries, you can compile it manually using unreal engine. This is usefull if you want to get the latest changes or if you want to make changes to the maps or the simulation itself. If you want to run the unreal engine project from source you will need unreal engine and visual studio 2019 . On Ubuntu you can skip the visual studio 2019 part, but you still need Unreal Engine.","title":"From source using the Unreal Engine Editor"},{"location":"getting-started/#1-get-the-repository","text":"You can either download the repo using the big green download button on the github page of this project or clone the repository. For cloning, checkout the documentation on this further down this page. Make sure you clone the repository in your home directory . When downloading or cloning, by default you get the latest, unreleased version. This is probably not the version that you want. Make sure you select the version that you need!","title":"1. Get the repository"},{"location":"getting-started/#2-compiling-the-airsim-plugin","text":"The Unreal Engine project requires the AirSim plugin. We have to compile this plugin first. The AirSim plugin is made up of AirLib (/AirSim/AirLib) and the plugin code (/UE4Project/Plugins/AirSim/Source). AirLib is the shared code between the ros wrapper and the AirSim Unreal Engine plugin. First build the AirLib code. On Windows, open the Developer Command Prompt for VS 2019 , go to Formula-Student-Driverless-Simulator/AirSim and run build.cmd On Ubuntu, go to folder AirSim and run setup.sh and build.sh . So what does build.cmd or setup.sh+build.sh do? It downloads any nessesary libraries and compiles AirLib. After compilation it places the files in the UE4Project so that these files can be used durcing compilation of the plugin. The first time this takes quite a while. Go walk around a bit, maybe start playing factoryidle .","title":"2. Compiling the AirSim plugin"},{"location":"getting-started/#3-working-with-the-unreal-engine-project","text":"Launch Unreal Engine, press Browse and open the FSDS project in ~/Driverless-Competition-Simulator/UE4Project/FSOnline.uproject . The project should now open correctly. If it does not, make sure of the following: you have cloned the repository inside your home folder (~/) you have cloned with LFS enabed. If not, run git lfs install and git lfs pull to download the large files. within ~/Driverless-Competition-Simulator/AirSim/ , you have run build.cmd on Windows and ./setup.sh && ./build.sh on Ubuntu. On Ubuntu, we recommend adding the following alias to your ~/.bashrc to speed up the process of opening the repository in the future: alias ue='~/UnrealEngine/Engine/Binaries/Linux/UE4Editor ~/Formula-Student-Driverless-Simulator/UE4Project/FSOnline.uproject' It might show an error like 'This project was made with a different version of the Unreal Engine'. In that case select more options and skip conversion . When asked to rebuild the 'Blocks' and 'AirSim' modules, choose 'Yes'. This is the step where the plugin part of AirSim is compiled. After some time Unreal Engine will start and you can launch the game. Run the game in standalone mode or or selected viewport mode, simulate and eject mode do not support camera's. If you make changes to AirLib you have to run build.cmd again. If you make changes to the plugin code or AirLib, you only have to recompile the plugin. This can be done from within the Unreal Editor. go to to Window -> Developer tools -> Modules . Search for AirSim and click Recompile .","title":"3. Working with the Unreal Engine project"},{"location":"getting-started/#4-launching-the-game","text":"To run the game, click the big Play button. If you want to run it like it would run when packaged, choose 'Run as standalone game'.","title":"4. Launching the game"},{"location":"getting-started/#next-steps","text":"Now you are ready to connect your autonomous system. At the moment there are two integration methods: Use the ROS bridge to connect your ROS autonomous system to the bridge. Use the Python client to interact with the simulator.","title":"Next steps"},{"location":"gps/","text":"GPS The GPS model in AirSim has been configured to mimic an average GPS receiver used during formula student competitions. The GPS operates at 10 Hz, this value can be altered with update_frequency parameter. The GPS is located in the vehicle pawn center . The GPS captures the position of the vehicle in the geodetic reference system, namely longitude [deg], latitude [deg], altitude [m]. At this moment there is no artificial latency added. Still there will be some delay between the creation of the gps points and arrival at the autonomous system because of network latency. In the future, this can be altered with update_frequency parameter. All GPS positions are timestamped. As soon as the car starts, gps becomes available at maximum resolution. There is no warmup time since during physical formula student events cars often start with pre-locked gps. The inaccuracies in the GPS sensor position is generated with adding an offset vector from the ground truth, as a vector [x_err, y_err, z_err] T , where x_err, y_err are generated through a Gaussian distribution with 0 mean and eph variance, and z_err through a Gaussian distribution with 0 mean and epv variance. Currently the eph is set to 4 cm . This noise, however, is only added if the measured velocity is above an arbitrarily chosen threshold (currently 0.1m/s). To velocities below that, this additional inaccuracy is not introduced to avoid \"jumpy\" positions during standstill of the vehicle. See GpsSimple.hpp (/AirSim/AirLib/include/sensors/gps/GpsSimple.hpp) and GpsSimpleParams.hpp for the implementation of the gps model. Add the gps to the car In your settings.json , add the following to the Sensors object within the vehicle configuration: \"Gps\" : { \"SensorType\": 3, \"Enabled\": true }, The key of the object, in this case Gps is the name of the sensor: this gps sensor is named Gps . Use this name to retrieve sensordata. At the moment no accuracy-configuration is supported. Python client # Args: # gps_name (str, optional): Name of GPS to get data from, specified in settings.json. When no name is provided the last gps will be used. # vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to. # Returns: # time_stamp (np.uint64): nanosecond timestamp of when the gps position was captured # gnss: # eph (float): Standard deviation of horizontal position error (meters) # epv (float): Standard deviation of vertical position error (meters) # geo_point: The altitude, latitude and longitude of the gps # latitude (float) # longitude (float) # altitude (float) # velocity (Vector3r): Velocity in three directions (x_val, y_val and z_val) in meter/second # time_utc (np.uint64): UTC millisecond timestamp of when the gps position was captured gps_data = client.getGpsData(gps_name = '', vehicle_name = 'FSCar') An example of how to collect gps data: import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() while True: gps = client.getGpsData() print(\"timestamp nano: \", gps.time_stamp) print(\"timestamp utc: \", gps.gnss.time_utc) print(\"eph: \", gps.gnss.eph) print(\"epv: \", gps.gnss.epv) print(\"geo point: \", gps.gnss.geo_point) print(\"velocity: \", gps.gnss.velocity) print() time.sleep(1) Full example here .","title":"GPS"},{"location":"gps/#gps","text":"The GPS model in AirSim has been configured to mimic an average GPS receiver used during formula student competitions. The GPS operates at 10 Hz, this value can be altered with update_frequency parameter. The GPS is located in the vehicle pawn center . The GPS captures the position of the vehicle in the geodetic reference system, namely longitude [deg], latitude [deg], altitude [m]. At this moment there is no artificial latency added. Still there will be some delay between the creation of the gps points and arrival at the autonomous system because of network latency. In the future, this can be altered with update_frequency parameter. All GPS positions are timestamped. As soon as the car starts, gps becomes available at maximum resolution. There is no warmup time since during physical formula student events cars often start with pre-locked gps. The inaccuracies in the GPS sensor position is generated with adding an offset vector from the ground truth, as a vector [x_err, y_err, z_err] T , where x_err, y_err are generated through a Gaussian distribution with 0 mean and eph variance, and z_err through a Gaussian distribution with 0 mean and epv variance. Currently the eph is set to 4 cm . This noise, however, is only added if the measured velocity is above an arbitrarily chosen threshold (currently 0.1m/s). To velocities below that, this additional inaccuracy is not introduced to avoid \"jumpy\" positions during standstill of the vehicle. See GpsSimple.hpp (/AirSim/AirLib/include/sensors/gps/GpsSimple.hpp) and GpsSimpleParams.hpp for the implementation of the gps model.","title":"GPS"},{"location":"gps/#add-the-gps-to-the-car","text":"In your settings.json , add the following to the Sensors object within the vehicle configuration: \"Gps\" : { \"SensorType\": 3, \"Enabled\": true }, The key of the object, in this case Gps is the name of the sensor: this gps sensor is named Gps . Use this name to retrieve sensordata. At the moment no accuracy-configuration is supported.","title":"Add the gps to the car"},{"location":"gps/#python-client","text":"# Args: # gps_name (str, optional): Name of GPS to get data from, specified in settings.json. When no name is provided the last gps will be used. # vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to. # Returns: # time_stamp (np.uint64): nanosecond timestamp of when the gps position was captured # gnss: # eph (float): Standard deviation of horizontal position error (meters) # epv (float): Standard deviation of vertical position error (meters) # geo_point: The altitude, latitude and longitude of the gps # latitude (float) # longitude (float) # altitude (float) # velocity (Vector3r): Velocity in three directions (x_val, y_val and z_val) in meter/second # time_utc (np.uint64): UTC millisecond timestamp of when the gps position was captured gps_data = client.getGpsData(gps_name = '', vehicle_name = 'FSCar') An example of how to collect gps data: import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() while True: gps = client.getGpsData() print(\"timestamp nano: \", gps.time_stamp) print(\"timestamp utc: \", gps.gnss.time_utc) print(\"eph: \", gps.gnss.eph) print(\"epv: \", gps.gnss.epv) print(\"geo point: \", gps.gnss.geo_point) print(\"velocity: \", gps.gnss.velocity) print() time.sleep(1) Full example here .","title":"Python client"},{"location":"ground-speed-sensor/","text":"Ground Speed Sensor (GSS) The ground speed sensor is modeled around the Kistler ground speed (like the Kistler Correvit SFII). Velocity information is captured in the global world frame in ENU frame. At this moment no extra noise is added to the sensordata since the kistler 250hz data averaged into the 100hz is so close to ground truth that adding noise would be unrealistic. Add the sensor to the car In your settings.json , add the following to the Sensors object within the vehicle configuration: \"GSS\" : { \"SensorType\": 7, \"Enabled\": true } A car can have no more then 1 gss. Therefore, the name of the gss (in this example GSS ) doesn't matter; for requesting gss data you don't need to pass a sensorname. Python # Args: # vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to. # Returns: # time_stamp (np.uint64): nanosecond timestamp of when the gss data was captured # linear_velocity (Vector3r): velocity in m/s of the car in world reference frame gss_data = client.getGroundSpeedSensorData(vehicle_name = 'FSCar') An example of how to collect gss data: import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() while True: gss = client.getGroundSpeedSensorData(vehicle_name='FSCar') print(\"timestamp nano: \", gss.time_stamp) print(\"linear_velocity: \", gss.linear_velocity) print() time.sleep(1) Full example here . ROS When using the ROS bridge, ground speed sensordata will be published on /fsds/gss with the geometry_msgs/TwistStamped message type. Make sure you have added the sensor to your settings.json file. Appart from the header fields, only x , y and z of the twist.linear are populated. header: seq: 5747 stamp: secs: 1595325426 nsecs: 617730500 frame_id: \"fsds/FSCar\" twist: linear: x: 4.80838251114 y: -0.0 z: -0.0214105024934 angular: x: 0.0 y: 0.0 z: 0.0","title":"GSS"},{"location":"ground-speed-sensor/#ground-speed-sensor-gss","text":"The ground speed sensor is modeled around the Kistler ground speed (like the Kistler Correvit SFII). Velocity information is captured in the global world frame in ENU frame. At this moment no extra noise is added to the sensordata since the kistler 250hz data averaged into the 100hz is so close to ground truth that adding noise would be unrealistic.","title":"Ground Speed Sensor (GSS)"},{"location":"ground-speed-sensor/#add-the-sensor-to-the-car","text":"In your settings.json , add the following to the Sensors object within the vehicle configuration: \"GSS\" : { \"SensorType\": 7, \"Enabled\": true } A car can have no more then 1 gss. Therefore, the name of the gss (in this example GSS ) doesn't matter; for requesting gss data you don't need to pass a sensorname.","title":"Add the sensor to the car"},{"location":"ground-speed-sensor/#python","text":"# Args: # vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to. # Returns: # time_stamp (np.uint64): nanosecond timestamp of when the gss data was captured # linear_velocity (Vector3r): velocity in m/s of the car in world reference frame gss_data = client.getGroundSpeedSensorData(vehicle_name = 'FSCar') An example of how to collect gss data: import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() while True: gss = client.getGroundSpeedSensorData(vehicle_name='FSCar') print(\"timestamp nano: \", gss.time_stamp) print(\"linear_velocity: \", gss.linear_velocity) print() time.sleep(1) Full example here .","title":"Python"},{"location":"ground-speed-sensor/#ros","text":"When using the ROS bridge, ground speed sensordata will be published on /fsds/gss with the geometry_msgs/TwistStamped message type. Make sure you have added the sensor to your settings.json file. Appart from the header fields, only x , y and z of the twist.linear are populated. header: seq: 5747 stamp: secs: 1595325426 nsecs: 617730500 frame_id: \"fsds/FSCar\" twist: linear: x: 4.80838251114 y: -0.0 z: -0.0214105024934 angular: x: 0.0 y: 0.0 z: 0.0","title":"ROS"},{"location":"how-to-release/","text":"How to release a new version Export the Unreal Engine project for release Open the UE4Project in the Unreal Editor Ensure 'File' -> 'Package Project' -> 'Build configuration' is set to 'Shipping, Choose 'File' -> 'Package Project' -> 'Windows (64 bit)' Select any folder on your computer. Wait until it finishes. Go into the WindowsNoEditor folder and rename Blocks.exe to FSDS.exe Zip all files and upload to github release! Deploying documentation For documentation we use mkdocs and mike . Hosting is provided by github pages. To tag a new version of the documentation and release it to github, first checkout the version that you want to deploy. Then run mike deploy VERSION latest -u -p . This will compile the documentation, store it as a new version in the gh-pages branch, update the latest alias to point at the new version and push the gh-pages branch to github and thus making the documentation public. To create a new version without updating the latest tag, omit the latest -u part.","title":"How to release"},{"location":"how-to-release/#how-to-release-a-new-version","text":"","title":"How to release a new version"},{"location":"how-to-release/#export-the-unreal-engine-project-for-release","text":"Open the UE4Project in the Unreal Editor Ensure 'File' -> 'Package Project' -> 'Build configuration' is set to 'Shipping, Choose 'File' -> 'Package Project' -> 'Windows (64 bit)' Select any folder on your computer. Wait until it finishes. Go into the WindowsNoEditor folder and rename Blocks.exe to FSDS.exe Zip all files and upload to github release!","title":"Export the Unreal Engine project for release"},{"location":"how-to-release/#deploying-documentation","text":"For documentation we use mkdocs and mike . Hosting is provided by github pages. To tag a new version of the documentation and release it to github, first checkout the version that you want to deploy. Then run mike deploy VERSION latest -u -p . This will compile the documentation, store it as a new version in the gh-pages branch, update the latest alias to point at the new version and push the gh-pages branch to github and thus making the documentation public. To create a new version without updating the latest tag, omit the latest -u part.","title":"Deploying documentation"},{"location":"import-car-3d-model/","text":"How to import your car 3d model Import your own 3d model of your formula student car into the world and use it during simulation! Preface (a friendly warning) Without experience with blender and unreal engine this whole process is painfull. There are waaaay to many options and buttons in these tools and it never does what you want to do. If you want your 3d model to be added to this repository you can create a github ticket and ask verry nicely if someone wants to help. Maybe you find someone with some experience that will help you. Make sure you have your fbx model prepared beforehand (see below). Are you a great warrior forged in the fires of a collapsing star seeking for the ultimate challenge? Let me introduce you to the world of importing 3d model. 1. Prepare your 3d model First we have to get the 3d model prepared and exported to fbx format. Most 3d editing software support this format. This tutorial however only describes blender becuase it's free (yay) and relatively easy to use. Build your car and get it to the following structure: There should be 1 object for the chassy and 4 child objects for the wheels. You do not need to define bones, those will be auto-created when importing to unreal. Give the wheels easy names because we will be using those names later on. Note: the car should be facing the positive x axis in blender. This is also the time to paint your car. Add surfuces, create textures and go crazy with paint. Export any textures to image format. Before we export the model, check that all deltra transforms have been applied and all scales are set to 1 . Also don't forget to recalculate normals Export the 3d model to FBX format using all default settings. Ensure you are only exporting meshes and armature. 2. Importing the model into unreal engine Create a new folder in AirSim Content/VehicleADV/Cars/ with the name of your car. Use all defaults, except: Check Skeletal Mesh Import content type: Geometry Only Skeleton 'None' Now you should see a mesh file, physics asset and skeleton. 3. Styling your model Go into the mesh file to style your car. There are many ways to make your mesh fancy using materials and textures. Good luck Googling. 4. Configuring the physics asset The physics asset defines which part of the vehicle interacts with the world. It is basically the collision model. If you are not planning to use this car in an online competition then you can configure the bounding boxes as close to the mesh as you want. Some key settings to keep in mind: Each wheel bone needs it's own physics body. All wheel bodies need to be Physics Type 'Kinematic' Only the wheels should touch the ground. Take a look at Suv_Pa for an example: Below picture shows how bounding boxes can be bigger or smaller than the mesh. All bounding boxes will be checked by: placing each vehicle into the virtual world using a SkeletalMeshActor Going into top view mode (Alt-J) Enabling Collision visualisation (Alt-C) Using the middle-mouse button measuring tool to check the width and length The following physics asset are examples of above specification: AirSim/Content/VehicleAdv/Cars/TechnionCar/RacingTechnion_PA.uasset AirSim/Content/VehicleAdv/Cars/ReferenceCar/ReferenceCar_Pa.uasset AirSim/Content/VehicleAdv/Cars/SuvCar/RacingSuv_Pa.uasset 5. Create an animation Go into your car folder and create a new Animation Blueprint. Choose VehicleAnimInstance as parent class and choose your cars skeleton as target. Now make it look like this: 6. Create a pawn Go into your car folder and create create a new Blueprint Class with base class CarPawn. This will be the actual pawn that will be controlled by the autonomous system. In the components browser, select 'Mesh'. Next, in the details pane, set 'Skeletal Mesh' to your car. Set 'Anim Class' to the animation class you created in step 5. In the components browser, select 'VehicleMovement' Next, in the details pane: Set Mass to 255,0 In the Wheel Setups set the first two Wheel Class to FormulaFrontWheel and the last two Wheel Class to FormulaBackWheel In Wheel Setups set the Bone Name of each wheel to the corresponding bone name as found in the skeleton. From top to bottom they are front left, front right, back left, back right From Drag Coefficient onwards all settings should be exactly equal to the settings in SuvCarPawn . Have fun copy-pasting. 7. Selecting a car Update your settings.json PawnPaths -> DefaultCar -> PawnBP field in the settings.json to point at the newly created pawn. Currently the following values are supported: Class'/AirSim/VehicleAdv/Cars/TechnionCar/TechnionCarPawn.TechnionCarPawn_C' Class'/AirSim/VehicleAdv/Cars/SuvCar/SuvCarPawn.SuvCarPawn_C' 8. Sharing your vehicle with the world! Wouldn't it be amazing if other people could enjoy your team's vehicle 3d model as well? You are welcome to create a pull request with your vehicle and it might be added to the base project repository! Submit the new model as a pull request which is up to date with the current project state, with the vehicle model and pawn files added. Contribution will be given in the readme of the project. Please note that only GPLv2 licensed material is accepted into this repository.","title":"Import your 3d car model"},{"location":"import-car-3d-model/#how-to-import-your-car-3d-model","text":"Import your own 3d model of your formula student car into the world and use it during simulation!","title":"How to import your car 3d model"},{"location":"import-car-3d-model/#preface-a-friendly-warning","text":"Without experience with blender and unreal engine this whole process is painfull. There are waaaay to many options and buttons in these tools and it never does what you want to do. If you want your 3d model to be added to this repository you can create a github ticket and ask verry nicely if someone wants to help. Maybe you find someone with some experience that will help you. Make sure you have your fbx model prepared beforehand (see below). Are you a great warrior forged in the fires of a collapsing star seeking for the ultimate challenge? Let me introduce you to the world of importing 3d model.","title":"Preface (a friendly warning)"},{"location":"import-car-3d-model/#1-prepare-your-3d-model","text":"First we have to get the 3d model prepared and exported to fbx format. Most 3d editing software support this format. This tutorial however only describes blender becuase it's free (yay) and relatively easy to use. Build your car and get it to the following structure: There should be 1 object for the chassy and 4 child objects for the wheels. You do not need to define bones, those will be auto-created when importing to unreal. Give the wheels easy names because we will be using those names later on. Note: the car should be facing the positive x axis in blender. This is also the time to paint your car. Add surfuces, create textures and go crazy with paint. Export any textures to image format. Before we export the model, check that all deltra transforms have been applied and all scales are set to 1 . Also don't forget to recalculate normals Export the 3d model to FBX format using all default settings. Ensure you are only exporting meshes and armature.","title":"1. Prepare your 3d model"},{"location":"import-car-3d-model/#2-importing-the-model-into-unreal-engine","text":"Create a new folder in AirSim Content/VehicleADV/Cars/ with the name of your car. Use all defaults, except: Check Skeletal Mesh Import content type: Geometry Only Skeleton 'None' Now you should see a mesh file, physics asset and skeleton.","title":"2. Importing the model into unreal engine"},{"location":"import-car-3d-model/#3-styling-your-model","text":"Go into the mesh file to style your car. There are many ways to make your mesh fancy using materials and textures. Good luck Googling.","title":"3. Styling your model"},{"location":"import-car-3d-model/#4-configuring-the-physics-asset","text":"The physics asset defines which part of the vehicle interacts with the world. It is basically the collision model. If you are not planning to use this car in an online competition then you can configure the bounding boxes as close to the mesh as you want. Some key settings to keep in mind: Each wheel bone needs it's own physics body. All wheel bodies need to be Physics Type 'Kinematic' Only the wheels should touch the ground. Take a look at Suv_Pa for an example: Below picture shows how bounding boxes can be bigger or smaller than the mesh. All bounding boxes will be checked by: placing each vehicle into the virtual world using a SkeletalMeshActor Going into top view mode (Alt-J) Enabling Collision visualisation (Alt-C) Using the middle-mouse button measuring tool to check the width and length The following physics asset are examples of above specification: AirSim/Content/VehicleAdv/Cars/TechnionCar/RacingTechnion_PA.uasset AirSim/Content/VehicleAdv/Cars/ReferenceCar/ReferenceCar_Pa.uasset AirSim/Content/VehicleAdv/Cars/SuvCar/RacingSuv_Pa.uasset","title":"4. Configuring the physics asset"},{"location":"import-car-3d-model/#5-create-an-animation","text":"Go into your car folder and create a new Animation Blueprint. Choose VehicleAnimInstance as parent class and choose your cars skeleton as target. Now make it look like this:","title":"5. Create an animation"},{"location":"import-car-3d-model/#6-create-a-pawn","text":"Go into your car folder and create create a new Blueprint Class with base class CarPawn. This will be the actual pawn that will be controlled by the autonomous system. In the components browser, select 'Mesh'. Next, in the details pane, set 'Skeletal Mesh' to your car. Set 'Anim Class' to the animation class you created in step 5. In the components browser, select 'VehicleMovement' Next, in the details pane: Set Mass to 255,0 In the Wheel Setups set the first two Wheel Class to FormulaFrontWheel and the last two Wheel Class to FormulaBackWheel In Wheel Setups set the Bone Name of each wheel to the corresponding bone name as found in the skeleton. From top to bottom they are front left, front right, back left, back right From Drag Coefficient onwards all settings should be exactly equal to the settings in SuvCarPawn . Have fun copy-pasting.","title":"6. Create a pawn"},{"location":"import-car-3d-model/#7-selecting-a-car","text":"Update your settings.json PawnPaths -> DefaultCar -> PawnBP field in the settings.json to point at the newly created pawn. Currently the following values are supported: Class'/AirSim/VehicleAdv/Cars/TechnionCar/TechnionCarPawn.TechnionCarPawn_C' Class'/AirSim/VehicleAdv/Cars/SuvCar/SuvCarPawn.SuvCarPawn_C'","title":"7. Selecting a car"},{"location":"import-car-3d-model/#8-sharing-your-vehicle-with-the-world","text":"Wouldn't it be amazing if other people could enjoy your team's vehicle 3d model as well? You are welcome to create a pull request with your vehicle and it might be added to the base project repository! Submit the new model as a pull request which is up to date with the current project state, with the vehicle model and pawn files added. Contribution will be given in the readme of the project. Please note that only GPLv2 licensed material is accepted into this repository.","title":"8. Sharing your vehicle with the world!"},{"location":"imu/","text":"IMU Sensor The IMU captures the acceleration, orientation and angular rate of the vehicle. The IMU sensor in FSDS is using AirSim's built in IMU sensor simulation, that has been modelled and parametrized according to MPU 6000 IMU from InvenSense . The maximum achievable internal IMU frequency is 1000 Hz, ouputing information of the vehicle's 3-axis angular velocity, 3-axis linear acceleration, as well as its orientation in quaternions. Keep in mind that at the moment the car position inside the simulation is updated at the simulator's framerate. IMU noise is added every time the IMU frame is requested. Requesting imu frequency at a higher rate then the simulators framerate will result in different data every time but all based on the same ground truth position. IMU gyroscope and accelomter bias and accuracy parameters can be found and fine-tuned in AirLib/include/sensors/imu/ImuSimpleParams.hpp. There is an open issue to make these configurable throught the settings.json . The angular velocity, linear acceleration outputs as well as their biases have artificially introduced Gaussian noise (0 mean, standard deviation of 1) updated on each IMU cycle. All of the IMU measurements are timestamped. The IMU is located in the vehicle pawn center . See ImuSimple.hpp (/AirSim/AirLib/include/sensors/imu/ImuSimple.hpp) and [ImuSimpleParams.hpp] /AirSim/AirLib/include/sensors/imu/ImuSimpleParams.hpp for the implementation of the IMU model. Add an IMU to the car In your settings.json , add the following to the Sensors object within the vehicle configuration: \"Imu\" : { \"SensorType\": 2, \"Enabled\": true }, The key of the object, in this case Imu is the name of the sensor: this imu sensor is named Imu . Use this name to retrieve sensordata. At the moment no configuration of noise is supported. Python client # Args: # imu_name (str, optional): Name of IMU to get data from, specified in settings.json. When no name is provided the last imu will be used. # vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to. # # Returns: # time_stamp (np.uint64) nanosecond timestamp of when the imu data was captured # orientation (Quaternionr) rotation of the sensor, relative to the northpole. It's like a compass # angular_velocity (Vector3r) how fast the car is rotating along it's three axis in radians/second # linear_acceleration (Vector3r) how fast the car is accelerating in meters/s2^m # imu_data = client.getImuData(imu_name = '', vehicle_name = 'FSCar') An example of how to collect imu data: import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() while True: imu = client.getImuData(imu_name = 'Imu', vehicle_name = 'FSCar') print(\"timestamp nano: \", imu.time_stamp) print(\"orientation: \", imu.orientation) print(\"angular velocity: \", imu.angular_velocity) print(\"linear acceleration: \", imu.linear_acceleration) print() time.sleep(1) Full example here .","title":"IMU"},{"location":"imu/#imu-sensor","text":"The IMU captures the acceleration, orientation and angular rate of the vehicle. The IMU sensor in FSDS is using AirSim's built in IMU sensor simulation, that has been modelled and parametrized according to MPU 6000 IMU from InvenSense . The maximum achievable internal IMU frequency is 1000 Hz, ouputing information of the vehicle's 3-axis angular velocity, 3-axis linear acceleration, as well as its orientation in quaternions. Keep in mind that at the moment the car position inside the simulation is updated at the simulator's framerate. IMU noise is added every time the IMU frame is requested. Requesting imu frequency at a higher rate then the simulators framerate will result in different data every time but all based on the same ground truth position. IMU gyroscope and accelomter bias and accuracy parameters can be found and fine-tuned in AirLib/include/sensors/imu/ImuSimpleParams.hpp. There is an open issue to make these configurable throught the settings.json . The angular velocity, linear acceleration outputs as well as their biases have artificially introduced Gaussian noise (0 mean, standard deviation of 1) updated on each IMU cycle. All of the IMU measurements are timestamped. The IMU is located in the vehicle pawn center . See ImuSimple.hpp (/AirSim/AirLib/include/sensors/imu/ImuSimple.hpp) and [ImuSimpleParams.hpp] /AirSim/AirLib/include/sensors/imu/ImuSimpleParams.hpp for the implementation of the IMU model.","title":"IMU Sensor"},{"location":"imu/#add-an-imu-to-the-car","text":"In your settings.json , add the following to the Sensors object within the vehicle configuration: \"Imu\" : { \"SensorType\": 2, \"Enabled\": true }, The key of the object, in this case Imu is the name of the sensor: this imu sensor is named Imu . Use this name to retrieve sensordata. At the moment no configuration of noise is supported.","title":"Add an IMU to the car"},{"location":"imu/#python-client","text":"# Args: # imu_name (str, optional): Name of IMU to get data from, specified in settings.json. When no name is provided the last imu will be used. # vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to. # # Returns: # time_stamp (np.uint64) nanosecond timestamp of when the imu data was captured # orientation (Quaternionr) rotation of the sensor, relative to the northpole. It's like a compass # angular_velocity (Vector3r) how fast the car is rotating along it's three axis in radians/second # linear_acceleration (Vector3r) how fast the car is accelerating in meters/s2^m # imu_data = client.getImuData(imu_name = '', vehicle_name = 'FSCar') An example of how to collect imu data: import time import fsds # connect to the AirSim simulator client = fsds.FSDSClient() # Check network connection client.confirmConnection() while True: imu = client.getImuData(imu_name = 'Imu', vehicle_name = 'FSCar') print(\"timestamp nano: \", imu.time_stamp) print(\"orientation: \", imu.orientation) print(\"angular velocity: \", imu.angular_velocity) print(\"linear acceleration: \", imu.linear_acceleration) print() time.sleep(1) Full example here .","title":"Python client"},{"location":"joystick/","text":"Joystick Controller Node If you are interested in driving the car around to gather training data without having to rely on your autonomous system, you can use an XBox controller to do so. Make sure you have built the ROS workspace . Simply run: cd ros source devel/setup.bash roslaunch joystick joystick.launch This node gets input from a joystick xbox controller (see http://wiki.ros.org/joy) and sends the values to the lowlevel controls (hardware) on topic /fsds_ros_bridge/FSCar/control_command The right trigger (RT) controls the acceleration (gas) The left trigger (LT) controls brake (negative acceleration). The x-axis of the left stick controlls the steering angle. Pressing and holding the B button enables boost mode. When the joy driver initializes it sometimes sends very high values. This would cause the car to move unpredictably. Therefore the controller is 'locked' when it is starts and when the controller is re-plugged in. To unlock both triggers must be fully pressed (gas and brake) after which commands will be sent. This node checks if the controller is plugged in by polling the linux device filename. If the controller is unplugged the file disapears and this node will start sending break. When the controller is locked or the controller is unplugged the node will continuously sending break setpoints at 10hz. During normal operation the maximum setpoints are limited quite a bit to make the car better controllable. If you enable boost mode (press B) you get more power and the car will move faster. To configure the vlaue mappings between xbox controller and car setpoints, go into src/joystick.cpp and change the value of the variables. HELP it doesn't work Make sure you run this on a computer that has the xbox controller attached! You can check if the controller is available by running the below command. $ sudo ls /dev/input/js0 crwxrwxrwx 1 root 993 13, 0 Nov 8 14:43 /dev/input/js0 If you get permisison erros you have to give ROS more permissions. Run sudo chmod 777 /dev/input/js0 If the device is connected but not available at js0 it might be mapped to another device. Find the correct device by running sudo ls -al /dev/input/js* When you find the correct device mapping, update the launchfile accordingly. The message [ERROR] Couldn't open joystick force feedback! is normal. Nothing to worry about. It is a warning that always happens with wired xbox controllers. testing To test this node on your computer just attach an xbox controller and run it as described above. Now chek the /fsds_ros_bridge/FSCar/control_command topic and you should see values corresponding to your controller movements. You can debug the input values from the joy driver by checking the /joy topic. Subscribers: /joy sensor_msgs/Joy Listens to joystick input which is then mapped to the control command msg. The mapping should feel intuitive but in case something is unclear, it is described in detail in /ros/src/fsds_ros_bridge/src/joystick.cpp Publishers: /fsds_ros_bridge/VEHICLE_NAME/control_command fs_msgs/ControlCommand","title":"Joystick"},{"location":"joystick/#joystick-controller-node","text":"If you are interested in driving the car around to gather training data without having to rely on your autonomous system, you can use an XBox controller to do so. Make sure you have built the ROS workspace . Simply run: cd ros source devel/setup.bash roslaunch joystick joystick.launch This node gets input from a joystick xbox controller (see http://wiki.ros.org/joy) and sends the values to the lowlevel controls (hardware) on topic /fsds_ros_bridge/FSCar/control_command The right trigger (RT) controls the acceleration (gas) The left trigger (LT) controls brake (negative acceleration). The x-axis of the left stick controlls the steering angle. Pressing and holding the B button enables boost mode. When the joy driver initializes it sometimes sends very high values. This would cause the car to move unpredictably. Therefore the controller is 'locked' when it is starts and when the controller is re-plugged in. To unlock both triggers must be fully pressed (gas and brake) after which commands will be sent. This node checks if the controller is plugged in by polling the linux device filename. If the controller is unplugged the file disapears and this node will start sending break. When the controller is locked or the controller is unplugged the node will continuously sending break setpoints at 10hz. During normal operation the maximum setpoints are limited quite a bit to make the car better controllable. If you enable boost mode (press B) you get more power and the car will move faster. To configure the vlaue mappings between xbox controller and car setpoints, go into src/joystick.cpp and change the value of the variables.","title":"Joystick Controller Node"},{"location":"joystick/#help-it-doesnt-work","text":"Make sure you run this on a computer that has the xbox controller attached! You can check if the controller is available by running the below command. $ sudo ls /dev/input/js0 crwxrwxrwx 1 root 993 13, 0 Nov 8 14:43 /dev/input/js0 If you get permisison erros you have to give ROS more permissions. Run sudo chmod 777 /dev/input/js0 If the device is connected but not available at js0 it might be mapped to another device. Find the correct device by running sudo ls -al /dev/input/js* When you find the correct device mapping, update the launchfile accordingly. The message [ERROR] Couldn't open joystick force feedback! is normal. Nothing to worry about. It is a warning that always happens with wired xbox controllers.","title":"HELP it doesn't work"},{"location":"joystick/#testing","text":"To test this node on your computer just attach an xbox controller and run it as described above. Now chek the /fsds_ros_bridge/FSCar/control_command topic and you should see values corresponding to your controller movements. You can debug the input values from the joy driver by checking the /joy topic.","title":"testing"},{"location":"joystick/#subscribers","text":"/joy sensor_msgs/Joy Listens to joystick input which is then mapped to the control command msg. The mapping should feel intuitive but in case something is unclear, it is described in detail in /ros/src/fsds_ros_bridge/src/joystick.cpp","title":"Subscribers:"},{"location":"joystick/#publishers","text":"/fsds_ros_bridge/VEHICLE_NAME/control_command fs_msgs/ControlCommand","title":"Publishers:"},{"location":"lidar/","text":"Lidar Adding a lidar to the vehicle. The lidar sensors are configured in the setting.json. This is an example lidar: \"Lidar1\": { \"SensorType\": 6, \"Enabled\": true, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"NumberOfLasers\": 7, \"PointsPerScan\": 2000, \"RotationsPerSecond\": 20, \"VerticalFOVUpper\": 0, \"VerticalFOVLower\": -25, \"HorizontalFOVStart\": 0, \"HorizontalFOVEnd\": 90, \"DrawDebugPoints\": false } Lidar1 is the name of the lidar. This name will be used to reference the camera when collecing the latest pointcloud. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in ENU frame. Roll , Pitch and Yaw are rotations in degrees. NumberOfLasers is the - duh - the number of lasers in the lidar. The lasers are stacked vertically and rotate on the horizontal plane. The lasers are distributed equally to cover the specified vertical field of view. Each laser has a range of 100 meters. The vertical field of view is specified by choosing the upper ( VerticalFOVUpper ) and lower ( VerticalFOVLower ) limit in degrees. The lower limit specifies the vertical angle between the horizontal plane of the lidar and the most bottom laser. The upper limit specifies the vertical angle between the horizontal plane of the lidar and most upper laser. The horizontal field of view of the lidar is specified with an upper ( HorizontalFOVStart ) and lower ( HorizontalFOVEnd ) limit in degree as well. The lower limit specifies the counterclockwise angle on a top view (negative yaw) from the direction the lidar is pointing towards. The upper limit specifies the clockwise angle on a top view (positive yaw) from the direction the lidar is pointing towards. The following image shows how the horizontal field of view start and end behave: RotationsPerSecond specifies how fast the lasers spins and how often a pointcloud is captured. There might be slight variations in the actual lidar frequency vs the configured rotation frequency. PointsPerScan is the number of firings per scan within the field of view. If all lasers hit, the returned pointcloud will contain this number of points in each pointcloud. DrawDebugPoints enables visualization of the lidar hits inside the unreal engine game. This is known to impact performance quite a bit. it is recommended to to only use this during debugging. Python lidardata = client.getLidarData(self, lidar_name = '', vehicle_name = 'FSCar') Calls the simulator API to retrieve the Lidar data. The API returns a pointcloud as a flat array of floats along with the timestamp of the capture and lidar pose. Lasers without a hit, shot into the air or too far away, are not included in the pointcloud. Every floats point represent [x, y, z] coordinate for each point hit within the range in the last scan, relative to the location of the lidar in meters. Args lidar_name (str, optional): Name of Lidar to get data from, specified in settings.json. With no name provided selects the last lidar in the settings.json. vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to, by default FSCar. Returns point_cloud (array of float): The points in the pointcloud. time_stamp (np.uint64): nanosecond timestamp of when the gps position was captured pose (Pose): position (Vector3r) and orientation (Quaternionr) of the location of the lidar at the moment of capture in global reference frame To convert the flat list of float points into a list of xyz coordinates, you can use the following example function: def parse_lidarData(self, point_cloud): \"\"\" Takes an array of float points and converts it into an array with 3-item arrays representing x, y and z \"\"\" points = numpy.array(point_cloud, dtype=numpy.dtype('f4')) return numpy.reshape(points, (int(points.shape[0]/3), 3)) points = parse_lidarData(lidardata.point_cloud) print(\"point 0 X: %f Y: %f Z: %f\" % (points[0][0], points[0][1], points[0][2])) Full example here .","title":"Lidar"},{"location":"lidar/#lidar","text":"","title":"Lidar"},{"location":"lidar/#adding-a-lidar-to-the-vehicle","text":"The lidar sensors are configured in the setting.json. This is an example lidar: \"Lidar1\": { \"SensorType\": 6, \"Enabled\": true, \"X\": 0, \"Y\": 0, \"Z\": -1, \"Roll\": 0, \"Pitch\": 0, \"Yaw\" : 0, \"NumberOfLasers\": 7, \"PointsPerScan\": 2000, \"RotationsPerSecond\": 20, \"VerticalFOVUpper\": 0, \"VerticalFOVLower\": -25, \"HorizontalFOVStart\": 0, \"HorizontalFOVEnd\": 90, \"DrawDebugPoints\": false } Lidar1 is the name of the lidar. This name will be used to reference the camera when collecing the latest pointcloud. X , Y and Z are the position of the lidar relative the vehicle pawn center of the car in ENU frame. Roll , Pitch and Yaw are rotations in degrees. NumberOfLasers is the - duh - the number of lasers in the lidar. The lasers are stacked vertically and rotate on the horizontal plane. The lasers are distributed equally to cover the specified vertical field of view. Each laser has a range of 100 meters. The vertical field of view is specified by choosing the upper ( VerticalFOVUpper ) and lower ( VerticalFOVLower ) limit in degrees. The lower limit specifies the vertical angle between the horizontal plane of the lidar and the most bottom laser. The upper limit specifies the vertical angle between the horizontal plane of the lidar and most upper laser. The horizontal field of view of the lidar is specified with an upper ( HorizontalFOVStart ) and lower ( HorizontalFOVEnd ) limit in degree as well. The lower limit specifies the counterclockwise angle on a top view (negative yaw) from the direction the lidar is pointing towards. The upper limit specifies the clockwise angle on a top view (positive yaw) from the direction the lidar is pointing towards. The following image shows how the horizontal field of view start and end behave: RotationsPerSecond specifies how fast the lasers spins and how often a pointcloud is captured. There might be slight variations in the actual lidar frequency vs the configured rotation frequency. PointsPerScan is the number of firings per scan within the field of view. If all lasers hit, the returned pointcloud will contain this number of points in each pointcloud. DrawDebugPoints enables visualization of the lidar hits inside the unreal engine game. This is known to impact performance quite a bit. it is recommended to to only use this during debugging.","title":"Adding a lidar to the vehicle."},{"location":"lidar/#python","text":"lidardata = client.getLidarData(self, lidar_name = '', vehicle_name = 'FSCar') Calls the simulator API to retrieve the Lidar data. The API returns a pointcloud as a flat array of floats along with the timestamp of the capture and lidar pose. Lasers without a hit, shot into the air or too far away, are not included in the pointcloud. Every floats point represent [x, y, z] coordinate for each point hit within the range in the last scan, relative to the location of the lidar in meters. Args lidar_name (str, optional): Name of Lidar to get data from, specified in settings.json. With no name provided selects the last lidar in the settings.json. vehicle_name (str, optional): Name of vehicle to which the sensor corresponds to, by default FSCar. Returns point_cloud (array of float): The points in the pointcloud. time_stamp (np.uint64): nanosecond timestamp of when the gps position was captured pose (Pose): position (Vector3r) and orientation (Quaternionr) of the location of the lidar at the moment of capture in global reference frame To convert the flat list of float points into a list of xyz coordinates, you can use the following example function: def parse_lidarData(self, point_cloud): \"\"\" Takes an array of float points and converts it into an array with 3-item arrays representing x, y and z \"\"\" points = numpy.array(point_cloud, dtype=numpy.dtype('f4')) return numpy.reshape(points, (int(points.shape[0]/3), 3)) points = parse_lidarData(lidardata.point_cloud) print(\"point 0 X: %f Y: %f Z: %f\" % (points[0][0], points[0][1], points[0][2])) Full example here .","title":"Python"},{"location":"local-setup/","text":"Local setup If you are planning to use 2 PC's, 1 Windows machine running the simulator and another machine running Linux, there are some things you should know. Python autonomous system If your autonomous system is written in python and you only have 1 option at the moment to run the simulator with 2 local machines. This is by running the fsds-bridge on the Linux machine. This means that you will not be able to ensure low latency to and from your Windows machine running the simulator. But running the simulator will be exactly the same as in the getting started page . C++ autonomous system If your system is written in C++, you have 2 options: - Running the fsds-bridge on your Linux machine \u2192 more latency between ROS bridge and simulator - Running the fsds-bridge on your Windows machine in WSL \u2192 Running the fsds/controlCommand topic in UDP mode, might lose some packets If you are using WSL 2 and you manage to get ROS to work with 2 machines, please help us understand how by writing a comment on this issue. Network setup If you have never run ROS with multiple machines, follow this tutorial and this one on the ROS Wiki. If everything is setup correctly you should be able to run the bridge with the UDP_control argument. So you would launch the bridge as follows: roslaunch fsds_ros_bridge fsds_ros_bridge.launch UDP_control:=true The changes have been tested on WSL 1 (Ubuntu 18.04), second machine (Ubuntu 18.04) with ROS melodic on both machines.","title":"Local setup"},{"location":"local-setup/#local-setup","text":"If you are planning to use 2 PC's, 1 Windows machine running the simulator and another machine running Linux, there are some things you should know.","title":"Local setup"},{"location":"local-setup/#python-autonomous-system","text":"If your autonomous system is written in python and you only have 1 option at the moment to run the simulator with 2 local machines. This is by running the fsds-bridge on the Linux machine. This means that you will not be able to ensure low latency to and from your Windows machine running the simulator. But running the simulator will be exactly the same as in the getting started page .","title":"Python autonomous system"},{"location":"local-setup/#c-autonomous-system","text":"If your system is written in C++, you have 2 options: - Running the fsds-bridge on your Linux machine \u2192 more latency between ROS bridge and simulator - Running the fsds-bridge on your Windows machine in WSL \u2192 Running the fsds/controlCommand topic in UDP mode, might lose some packets If you are using WSL 2 and you manage to get ROS to work with 2 machines, please help us understand how by writing a comment on this issue.","title":"C++ autonomous system"},{"location":"local-setup/#network-setup","text":"If you have never run ROS with multiple machines, follow this tutorial and this one on the ROS Wiki. If everything is setup correctly you should be able to run the bridge with the UDP_control argument. So you would launch the bridge as follows: roslaunch fsds_ros_bridge fsds_ros_bridge.launch UDP_control:=true The changes have been tested on WSL 1 (Ubuntu 18.04), second machine (Ubuntu 18.04) with ROS melodic on both machines.","title":"Network setup"},{"location":"map-tutorial/","text":"FSDS Map Building Tutorial Building new maps for FSDS is very useful to test your Autonomous System under various conditions. Creating a map requires using AirSim and Unreal Engine Editor from the FSDS source code. Instructions to install these can be found here . The simplest way to get started is to copy and paste an existing map and modify it to your needs. The simple trainning map with no added decorations can be found here . The base map used in competition can be found this folder along with the maps used in the 2020 FSOnline competition which all include decorations. All assets required and used by other maps can be found in the the UE4Project/Content folder found here . Once an asset is placed in the map, it is then called an actor. All transforms are done using Unreal Units (uu) which relate to real metric units through the World to Meters value from the World Settings . All maps use a value of 100 meaning 1 uu = 1cm (or 100 uu = 1m). Placing Cones Start/Finish Line The StartFinishLine asset as it's name suggests implements a start and finish line in one. The asset can be found under Content/FormulaStudentAssets . The asset blueprints start a lap timer when the car first drives over a colision box within the asset. All other following colisions with that box stops the current lap timer, and starts a new one. Placing the Start/Finish line is as simple as dragging it from the content browser into the editor. You can move it around with your mouse using the translation axis with the actor selected in the World Outliner . Or enter in transform values in the actor's Details. You must then make sure that the added asset is linked to the referee for the map. The referee must also have it's values updated. See the Referee section for details. If any static object is in contact with the collision detection box, it will start the timer before you have first driven over it. To fix this, you can update the \"Spline Cones Center\" variable string to match the string of the actor colliding with the box. This is primarily used for the skidpad tracks, where cones exist on the collision boundary. Cone Splines The cone splines assets can be found in Content/RaceCourse/Model/Splines . There is a spline_cones asset used for placing blue and yellow cones or spline_cones_mini_orange used for small orange cones. You can place a cone spline by dragging it from the content browser into the editor. Moving the main translation axis or editing the transform of the self value of the actor's details will move the entire cone spline. To edit individual positions of the spline points, click on a spline point, then under the Spline (Inherited) details of the actor, the Selected Points will allow you to edit the position of the spline point relative to the actor's main transform location. Additional spline points can be added by right clickling on a spline point, selecting Duplicate Spline Point and moving the newly created point to the desired location. In the right click menu, you can also change the spline type (curve, linear or constant), snap it to the floor, or delete the point. The distance between blue and yellow cones will default to 3.5m with a default spacing between cones of 4m. These values can be changed in the actor's details under the self options. You must then make sure that the added asset is linked to the referee for the map. The referee must also have it's values updated. See the Referee section for details. An example spline_cones actor can be seen below. The first image shows the self details with the main transform menu, referee reference, and spacing options. The second image shows when a spline point is selected, with the Selected Points menu to edit the spline point. Finish Line The FinishLine asset is a special instance of the StartFinishLine asset. The asset can be found under Content/FormulaStudentAssets . This asset is used if the track is not a loop but linear, like the acceleration track, where the start and finish are at different locations. The blueprints of this asset calls the functions of the Start/Finish line asset to end that actor's timer when the vehicle collides with the box of the finish line. You must make sure that the added asset is linked to both the Start/Finish actor and referee. Failing to link to the Start/Finish actor will mean that lap times will not be properly stopped. See the Referee section for details about the referee link. Referee The RefereeBP asset is used to display cone hits, display lap times, pass cone locations and other messages to ros. The asset can be found under Content/FormulaStudentAssets . One referee must be placed in the map and linked to all cone actors in your map in order for it to function properly. The various reference links are described below with details about allowed links. At a minimum, the Cones field must link to an actor made of the spline_cones asset and the Track Orange Start must link to an actor made of the StartFinishLine asset. If other cone asset actors exist, not making the optional links means that then cone hits, printing the list of cones to ros, or other functions from that actor may not work properly. The links must be made to actors of the correct asset type as shown below. Link Variable Name Description Actor Asset Type Cones Required. This should be the main spline of your track with yellow and blue cones. spline_cones Track Orange Start Required. This will be your Start/Finish line of your map. StartFinishLine Track Orange Mini Optional. If you have a spline of mini orange cones, it should be linked here. This is used in the acceleration track for the finish area. spline_cones_mini_orange Track Orange Finish Optional. If using a seperate finish line, it must be linked here. This is used in the acceleration map. FinishLine Track Orange Mini Skidpad Optional. If you have a second spline of mini orange cones , it should be linked here. This is used in the skidpad map since 2 splines are needed for the start and finish areas. spline_cones_mini_orange Cones Skidpad 1,2,3 Optional. If your track is made of multiple yellow and blue cone splines, it should be linked here. These are used to create the skidpad track. spline_cones An example of the minimum required referee references can be seen below. Car Starting Position The starting position of the car on the map is determined by the PlayerStart actor. Through the transforms of the actor, it can be moved to any desired location and orientation on the track. This must be present for the vehicle to spawn. Weather and Day Cycle Conditions Tutorial not yet complete. \"How to set the weather and day cycle conditions. We don't want automatic environment changes so we must document how to set these to some static value.\" Other Decorations Other decorations and textures exist within the content folders to give your maps a better look. For example, there are spline_tires and spline_fences assests which can be used to place tires and fences on the tracks. There exists many banner adverts, bleachers and much more for you to play around with! There is also a laptime display asset called LaptimeDisplay found in the Content/FormulaStudentAssets folder. It will display the current lap time in the world. If used, you must be sure to update the actor's reference to the StartFinishLine asset placed in your world since it must know where to pull the time from.","title":"FSDS Map Building Tutorial"},{"location":"map-tutorial/#fsds-map-building-tutorial","text":"Building new maps for FSDS is very useful to test your Autonomous System under various conditions. Creating a map requires using AirSim and Unreal Engine Editor from the FSDS source code. Instructions to install these can be found here . The simplest way to get started is to copy and paste an existing map and modify it to your needs. The simple trainning map with no added decorations can be found here . The base map used in competition can be found this folder along with the maps used in the 2020 FSOnline competition which all include decorations. All assets required and used by other maps can be found in the the UE4Project/Content folder found here . Once an asset is placed in the map, it is then called an actor. All transforms are done using Unreal Units (uu) which relate to real metric units through the World to Meters value from the World Settings . All maps use a value of 100 meaning 1 uu = 1cm (or 100 uu = 1m).","title":"FSDS Map Building Tutorial"},{"location":"map-tutorial/#placing-cones","text":"","title":"Placing Cones"},{"location":"map-tutorial/#startfinish-line","text":"The StartFinishLine asset as it's name suggests implements a start and finish line in one. The asset can be found under Content/FormulaStudentAssets . The asset blueprints start a lap timer when the car first drives over a colision box within the asset. All other following colisions with that box stops the current lap timer, and starts a new one. Placing the Start/Finish line is as simple as dragging it from the content browser into the editor. You can move it around with your mouse using the translation axis with the actor selected in the World Outliner . Or enter in transform values in the actor's Details. You must then make sure that the added asset is linked to the referee for the map. The referee must also have it's values updated. See the Referee section for details. If any static object is in contact with the collision detection box, it will start the timer before you have first driven over it. To fix this, you can update the \"Spline Cones Center\" variable string to match the string of the actor colliding with the box. This is primarily used for the skidpad tracks, where cones exist on the collision boundary.","title":"Start/Finish Line"},{"location":"map-tutorial/#cone-splines","text":"The cone splines assets can be found in Content/RaceCourse/Model/Splines . There is a spline_cones asset used for placing blue and yellow cones or spline_cones_mini_orange used for small orange cones. You can place a cone spline by dragging it from the content browser into the editor. Moving the main translation axis or editing the transform of the self value of the actor's details will move the entire cone spline. To edit individual positions of the spline points, click on a spline point, then under the Spline (Inherited) details of the actor, the Selected Points will allow you to edit the position of the spline point relative to the actor's main transform location. Additional spline points can be added by right clickling on a spline point, selecting Duplicate Spline Point and moving the newly created point to the desired location. In the right click menu, you can also change the spline type (curve, linear or constant), snap it to the floor, or delete the point. The distance between blue and yellow cones will default to 3.5m with a default spacing between cones of 4m. These values can be changed in the actor's details under the self options. You must then make sure that the added asset is linked to the referee for the map. The referee must also have it's values updated. See the Referee section for details. An example spline_cones actor can be seen below. The first image shows the self details with the main transform menu, referee reference, and spacing options. The second image shows when a spline point is selected, with the Selected Points menu to edit the spline point.","title":"Cone Splines"},{"location":"map-tutorial/#finish-line","text":"The FinishLine asset is a special instance of the StartFinishLine asset. The asset can be found under Content/FormulaStudentAssets . This asset is used if the track is not a loop but linear, like the acceleration track, where the start and finish are at different locations. The blueprints of this asset calls the functions of the Start/Finish line asset to end that actor's timer when the vehicle collides with the box of the finish line. You must make sure that the added asset is linked to both the Start/Finish actor and referee. Failing to link to the Start/Finish actor will mean that lap times will not be properly stopped. See the Referee section for details about the referee link.","title":"Finish Line"},{"location":"map-tutorial/#referee","text":"The RefereeBP asset is used to display cone hits, display lap times, pass cone locations and other messages to ros. The asset can be found under Content/FormulaStudentAssets . One referee must be placed in the map and linked to all cone actors in your map in order for it to function properly. The various reference links are described below with details about allowed links. At a minimum, the Cones field must link to an actor made of the spline_cones asset and the Track Orange Start must link to an actor made of the StartFinishLine asset. If other cone asset actors exist, not making the optional links means that then cone hits, printing the list of cones to ros, or other functions from that actor may not work properly. The links must be made to actors of the correct asset type as shown below. Link Variable Name Description Actor Asset Type Cones Required. This should be the main spline of your track with yellow and blue cones. spline_cones Track Orange Start Required. This will be your Start/Finish line of your map. StartFinishLine Track Orange Mini Optional. If you have a spline of mini orange cones, it should be linked here. This is used in the acceleration track for the finish area. spline_cones_mini_orange Track Orange Finish Optional. If using a seperate finish line, it must be linked here. This is used in the acceleration map. FinishLine Track Orange Mini Skidpad Optional. If you have a second spline of mini orange cones , it should be linked here. This is used in the skidpad map since 2 splines are needed for the start and finish areas. spline_cones_mini_orange Cones Skidpad 1,2,3 Optional. If your track is made of multiple yellow and blue cone splines, it should be linked here. These are used to create the skidpad track. spline_cones An example of the minimum required referee references can be seen below.","title":"Referee"},{"location":"map-tutorial/#car-starting-position","text":"The starting position of the car on the map is determined by the PlayerStart actor. Through the transforms of the actor, it can be moved to any desired location and orientation on the track. This must be present for the vehicle to spawn.","title":"Car Starting Position"},{"location":"map-tutorial/#weather-and-day-cycle-conditions","text":"Tutorial not yet complete. \"How to set the weather and day cycle conditions. We don't want automatic environment changes so we must document how to set these to some static value.\"","title":"Weather and Day Cycle Conditions"},{"location":"map-tutorial/#other-decorations","text":"Other decorations and textures exist within the content folders to give your maps a better look. For example, there are spline_tires and spline_fences assests which can be used to place tires and fences on the tracks. There exists many banner adverts, bleachers and much more for you to play around with! There is also a laptime display asset called LaptimeDisplay found in the Content/FormulaStudentAssets folder. It will display the current lap time in the world. If used, you must be sure to update the actor's reference to the StartFinishLine asset placed in your world since it must know where to pull the time from.","title":"Other Decorations"},{"location":"operator/","text":"Operator The operator offers competition officials a way to control and keep track of what is happening in the simulation It consists of a webserver with web interface where the official can select a team, missions and track and then launch the simulator. DOO cones and lap times will be shown and stored on disk. The bridge launcher is a helper script to launch the ros bridge on a different computer. It launches the ros bridge with configuration set in the web interface. The operator is primarily used during competition by officials. You don't need this for development and testing. Refer to the getting started guide first. Team config The operator uses the team_config.json file located in the /config folder to load all team specific configuration settings into the simulator. This includes the name of each team and their car settings. Whenever the simulation is started via the operator's web interface, the selected team's car settings are written to the settings.json file located in the main folder of the repository. This allows the operator to quickly switch between teams during competition. Note that the team_config.json file included in this repository is an example file. Logs Whenever a mission starts, a log file is created in the /operator/logs folder. All logs received by the webserver will be written to this log file, as long as the mission is ongoing. Log files are named using the following naming convention: {datetime}_{team}_{mission}.txt Passwords & Security To ensure only authorized personal can controll the simulation a password is required for all actions. This password is configured inside the operator.py file in variable access_token . Yes, this is a hard-coded value because I couldn't be bothered doing configuration files for something that changes almost never. Please change the default value 1234567890 it before running a real competition. The spectator server should also be protected with a password as described in the spectator documentation . The operator writes the spectator password to the settings.json file when the simulation starts so it is not needed to configure the SpectatorPassword inside the team_config.json . Again, this is a hard-coded value in the operator.py , default set to password . Please change before competition. Running the operator The operator is built to work on Windows because most likely, the simulation will also run on windows. To install all python dependencies, run the following command inside the /operator folder: $ pip install -r requirements.txt Next, download a packaged version of the simulator Go to the releases and download the latest version. Extract the zip to the simulator folder at the root of this project. The result should be that the following files and folders exist inside the simulator folder: FSDS.exe FSOnline/ Engine/ Here is described how to export the Unreal project. To start the web server, run the following command in the operator folder: $ python operator.py By default, the web interface binds to all ip's of the machine on port 5000. Running the bridge launcher The ros bridge will most likely not run on the same computer as the simulator as the ros bridge works best on Ubuntu and the simulator best on Windows. It is discouraged to run the ros bridge on WSL during competition, see #199 . To help you quickly start the ros bridge with the configuration parameters set in the operator you can use the bridge launcher. This script will request the configuration from the operator using an http api call and launch the bridge accordingly. Before you run it, change the following variables inside the launchbridge.py script to reflect your setup: operator_url = \"http://10.164.0.3:5000\" operator_token = \"1234567890\" Next, run the script as follows: eval `./launchbridge.py` Why eval? Because I think it is a bad idea to have the ros bridge run as a sub-process of the bridge launcher. We don't want a problem in the bridge launcher to stop the bridge. We don't want deal with forwarding logs and signals. ./launchbridge.py executes as a whole, outputting bash commands to stdout. All stdout is interperted as bash commands and executed by eval. The rosbridge process is ran after the launchbridge finishes, which is kind-of neat. I know that this is by no way a perfect solution. So if you think you can do better I invite you to propose your solution in a github ticket and open a pr.","title":"Operator"},{"location":"operator/#operator","text":"The operator offers competition officials a way to control and keep track of what is happening in the simulation It consists of a webserver with web interface where the official can select a team, missions and track and then launch the simulator. DOO cones and lap times will be shown and stored on disk. The bridge launcher is a helper script to launch the ros bridge on a different computer. It launches the ros bridge with configuration set in the web interface. The operator is primarily used during competition by officials. You don't need this for development and testing. Refer to the getting started guide first.","title":"Operator"},{"location":"operator/#team-config","text":"The operator uses the team_config.json file located in the /config folder to load all team specific configuration settings into the simulator. This includes the name of each team and their car settings. Whenever the simulation is started via the operator's web interface, the selected team's car settings are written to the settings.json file located in the main folder of the repository. This allows the operator to quickly switch between teams during competition. Note that the team_config.json file included in this repository is an example file.","title":"Team config"},{"location":"operator/#logs","text":"Whenever a mission starts, a log file is created in the /operator/logs folder. All logs received by the webserver will be written to this log file, as long as the mission is ongoing. Log files are named using the following naming convention: {datetime}_{team}_{mission}.txt","title":"Logs"},{"location":"operator/#passwords-security","text":"To ensure only authorized personal can controll the simulation a password is required for all actions. This password is configured inside the operator.py file in variable access_token . Yes, this is a hard-coded value because I couldn't be bothered doing configuration files for something that changes almost never. Please change the default value 1234567890 it before running a real competition. The spectator server should also be protected with a password as described in the spectator documentation . The operator writes the spectator password to the settings.json file when the simulation starts so it is not needed to configure the SpectatorPassword inside the team_config.json . Again, this is a hard-coded value in the operator.py , default set to password . Please change before competition.","title":"Passwords &amp; Security"},{"location":"operator/#running-the-operator","text":"The operator is built to work on Windows because most likely, the simulation will also run on windows. To install all python dependencies, run the following command inside the /operator folder: $ pip install -r requirements.txt Next, download a packaged version of the simulator Go to the releases and download the latest version. Extract the zip to the simulator folder at the root of this project. The result should be that the following files and folders exist inside the simulator folder: FSDS.exe FSOnline/ Engine/ Here is described how to export the Unreal project. To start the web server, run the following command in the operator folder: $ python operator.py By default, the web interface binds to all ip's of the machine on port 5000.","title":"Running the operator"},{"location":"operator/#running-the-bridge-launcher","text":"The ros bridge will most likely not run on the same computer as the simulator as the ros bridge works best on Ubuntu and the simulator best on Windows. It is discouraged to run the ros bridge on WSL during competition, see #199 . To help you quickly start the ros bridge with the configuration parameters set in the operator you can use the bridge launcher. This script will request the configuration from the operator using an http api call and launch the bridge accordingly. Before you run it, change the following variables inside the launchbridge.py script to reflect your setup: operator_url = \"http://10.164.0.3:5000\" operator_token = \"1234567890\" Next, run the script as follows: eval `./launchbridge.py` Why eval? Because I think it is a bad idea to have the ros bridge run as a sub-process of the bridge launcher. We don't want a problem in the bridge launcher to stop the bridge. We don't want deal with forwarding logs and signals. ./launchbridge.py executes as a whole, outputting bash commands to stdout. All stdout is interperted as bash commands and executed by eval. The rosbridge process is ran after the launchbridge finishes, which is kind-of neat. I know that this is by no way a perfect solution. So if you think you can do better I invite you to propose your solution in a github ticket and open a pr.","title":"Running the bridge launcher"},{"location":"ros-bridge/","text":"FSDS ROS bridge A ROS wrapper over the AirSim C++ Car client library. This code is based on the original AirSim ROS wrapper for the Multirotor API and provides an interface between AirSim + Unreal Engine and your ROS-based autonomous system. The fsds_ros_bridge is supposed to be launched pointing at the Autonomous System's ROS master so that it can publish and subscribe to topics within the autonomous system. Physically this node should run on the airsim simulation server (that is the one that also runs the Unreal) project. The node connects to the AirSim plugin, periodically retrieves sensor data (images, lidar, imu, gps) and publishes it on ROS topics. It listens for car setpoints on other another and forwards these to the AirSim plugin. Nodes The fsds_ros_bridge.launch launches the following nodes: * /fsds/ros_bridge node responsible for IMU, GPS, lidar, vehicle setpoints and go/finish signals. * /fsds/camera/CAMERANAME node is run for each camera configured in the settings.json . The nodes are launched using the cameralauncher.py script. Published topics Topic name Description Message Rate (hz) /fsds/gps This the current GPS coordinates of the drone in airsim. Read all about the gps simulation model here . Data is in the fsds/FSCar frame. sensor_msgs/NavSatFix 10 /fsds/imu Velocity, orientation and acceleration information. Read all about the IMU model here . Data is in the fsds/FSCar (enu) frame. sensor_msgs/Imu 250 /fsds/gss Ground speed sensor provide linear velocity of the vehicle ( fsds/FSCar ). Velocity is m/s. geometry_msgs/TwistStamped 100 /fsds/testing_only/odom Ground truth car position and orientation in ENU frame about the CoG of the car ( fsds/FSCar ). The units are m for distance. The orientation are expressed in terms of quaternions. The message is in the fsds/map frame. This is a frame that is not (yet) used anywhere else and is just here so you can easely reference it if needed. nav_msgs/Odometry 250 /fsds/testing_only/track Ground truth cone position and color with respect to the starting location of the car in ENU. Currently this only publishes the initial position of cones that are part of the track spline. Any cones placed manually in the world are not published here. Additionally, the track is published once and the message is latched (meaning it is always available for a newly created subscriber). fs_msgs/Track Latched /fsds/testing_only/extra_info This topic contains extra information about the simulator. At the moment these are the doo_counter, that keeps track of the amount of cones that have been hit, and the times of the laps fs_msgs/ExtraInfo 1 /fsds/camera/CAMERA_NAME Camera images. See ./camera.md . sensor_msgs/Image ~18 /fsds/lidar/LIDARNAME Publishes the lidar points for each lidar sensor. All points are in the fsds/LIDARNAME frame. Transformations between the fsds/LIDARNAME and fsds/FSCar frame are being published regularly. More info on the lidar sensor can be found here sensor_msgs/PointCloud `RotationsPerSecond param in settings.json /fsds/signal/go GO signal that is sent every second by the ROS bridge.The car is only allowed to drive once this message has been received. If no GO signal is received for more than 4 seconds, the AS can assume that fsds_ros_bridge has been shut down. This message also includes the mission type and track. More info about signal topics can be found in the competition-signals guide fs_msgs/GoSignal 1 /tf_static See Coordinate frames and transforms tf2_msgs/TFMessage 1 Subscribed topics Topic name Description Message /fsds/control_command This message includes the dimensionless values throttle, steering and brake. Throttle and brake range from 0 to 1. For steering -1 steers full to the left and +1 steers full to the right. Maximum steering angle is by default 25 degrees. The contents of this message fill the essential parts of the msr::airlib::CarApiBase::CarControl struct. This is the only way to control the car when the airsim ROS client is connected (keyboard will no longer work!). fs_msgs/ControlCommand /fsds/signal/finished Finished signal that is sent by the AS to stop the mission. The ROS bridge will forward the signal to the operator which in turn will stop the ROS bridge and finish the run. fs_msgs/FinishedSignal Services /fsds/reset fs_msgs/Reset Resets car to start location. Units If a topic streams a standard ROS message (like sensor_msgs/Imu ) then the units will be the recommended units in the message documentation. Custom messages (from the fs_msgs package) use the units specified in the message documentation as well. If in doubt, interpret distances in meters, angles in radians and rates in m/s and rad/s, etc. Coordinate frames and transforms The primary frame is the fsds/FSCar frame, which is fixed at the center of the car following the ROS coordinate system convention . The center of the car is the Unreal Engine car pawn position, which in turn is also the center of gravity. The ROS bridge regularly publishes static transforms between the fsds/FSCar frame and each of the cameras and lidars. Naming of these frames is fsds/SENSORNAME . For example, a lidar named Example will publish it's points in the fsds/Example frame. The position and orientation of a camera named Test will become available in the frame /fsds/Test . The transforms published on the /tf_static topic are the transforms specified in the settings.json . Only static transforms within the vehicle are published. All positions and rotations published by the ROS bridge are in line with the ROS defaults . This is the same coordinate system as everything else within the simulator . Parameters /fsds/ros_bridge/update_gps_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10hz). Timer callback frequency for updating and publishing the gps sensordata. This value must be equal or higher to the update frequency of the sensor configured in the settings.json /fsds/ros_bridge/update_imu_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the imu messages. This value must be equal or higher to the minimual sample rate of the sensor configured in the settings.json /fsds/ros_bridge/update_gss_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.01 seconds (100hz). Timer callback frequency for updating and publishing the ground speed sensor messages. /fsds/ros_bridge/update_odom_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the odometry. /fsds/ros_bridge/publish_static_tf_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 1 seconds (1 hz). The frequency at which the static transforms are published. /fsds/ros_bridge/update_lidar_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10 hz). The frequency at which the lidar is publshed. /fsds/ros_bridge/competition_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false , during competition set to true If competition mode is enabled, the testing_only topics won't be available. /fsds/ros_bridge/manual_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false Do not enable vehicle api control. You can control the car using the keyboard in the simulator. /fsds/ros_bridge/UDP_control [bool] Set in $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false Force UDP connection for the fsds/controlCommand topic when running the fsds_ros_bridge in WSL Visualization This package contains some useful launch and config files which will help you in visualizing the data being streamed through the above topics. To open Rviz with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch rviz:=true . To open Multiplot with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch plot:=true . Monitoring Performance monitoring of the ROS Bridge is described here","title":"ROS Bridge"},{"location":"ros-bridge/#fsds-ros-bridge","text":"A ROS wrapper over the AirSim C++ Car client library. This code is based on the original AirSim ROS wrapper for the Multirotor API and provides an interface between AirSim + Unreal Engine and your ROS-based autonomous system. The fsds_ros_bridge is supposed to be launched pointing at the Autonomous System's ROS master so that it can publish and subscribe to topics within the autonomous system. Physically this node should run on the airsim simulation server (that is the one that also runs the Unreal) project. The node connects to the AirSim plugin, periodically retrieves sensor data (images, lidar, imu, gps) and publishes it on ROS topics. It listens for car setpoints on other another and forwards these to the AirSim plugin.","title":"FSDS ROS bridge"},{"location":"ros-bridge/#nodes","text":"The fsds_ros_bridge.launch launches the following nodes: * /fsds/ros_bridge node responsible for IMU, GPS, lidar, vehicle setpoints and go/finish signals. * /fsds/camera/CAMERANAME node is run for each camera configured in the settings.json . The nodes are launched using the cameralauncher.py script.","title":"Nodes"},{"location":"ros-bridge/#published-topics","text":"Topic name Description Message Rate (hz) /fsds/gps This the current GPS coordinates of the drone in airsim. Read all about the gps simulation model here . Data is in the fsds/FSCar frame. sensor_msgs/NavSatFix 10 /fsds/imu Velocity, orientation and acceleration information. Read all about the IMU model here . Data is in the fsds/FSCar (enu) frame. sensor_msgs/Imu 250 /fsds/gss Ground speed sensor provide linear velocity of the vehicle ( fsds/FSCar ). Velocity is m/s. geometry_msgs/TwistStamped 100 /fsds/testing_only/odom Ground truth car position and orientation in ENU frame about the CoG of the car ( fsds/FSCar ). The units are m for distance. The orientation are expressed in terms of quaternions. The message is in the fsds/map frame. This is a frame that is not (yet) used anywhere else and is just here so you can easely reference it if needed. nav_msgs/Odometry 250 /fsds/testing_only/track Ground truth cone position and color with respect to the starting location of the car in ENU. Currently this only publishes the initial position of cones that are part of the track spline. Any cones placed manually in the world are not published here. Additionally, the track is published once and the message is latched (meaning it is always available for a newly created subscriber). fs_msgs/Track Latched /fsds/testing_only/extra_info This topic contains extra information about the simulator. At the moment these are the doo_counter, that keeps track of the amount of cones that have been hit, and the times of the laps fs_msgs/ExtraInfo 1 /fsds/camera/CAMERA_NAME Camera images. See ./camera.md . sensor_msgs/Image ~18 /fsds/lidar/LIDARNAME Publishes the lidar points for each lidar sensor. All points are in the fsds/LIDARNAME frame. Transformations between the fsds/LIDARNAME and fsds/FSCar frame are being published regularly. More info on the lidar sensor can be found here sensor_msgs/PointCloud `RotationsPerSecond param in settings.json /fsds/signal/go GO signal that is sent every second by the ROS bridge.The car is only allowed to drive once this message has been received. If no GO signal is received for more than 4 seconds, the AS can assume that fsds_ros_bridge has been shut down. This message also includes the mission type and track. More info about signal topics can be found in the competition-signals guide fs_msgs/GoSignal 1 /tf_static See Coordinate frames and transforms tf2_msgs/TFMessage 1","title":"Published topics"},{"location":"ros-bridge/#subscribed-topics","text":"Topic name Description Message /fsds/control_command This message includes the dimensionless values throttle, steering and brake. Throttle and brake range from 0 to 1. For steering -1 steers full to the left and +1 steers full to the right. Maximum steering angle is by default 25 degrees. The contents of this message fill the essential parts of the msr::airlib::CarApiBase::CarControl struct. This is the only way to control the car when the airsim ROS client is connected (keyboard will no longer work!). fs_msgs/ControlCommand /fsds/signal/finished Finished signal that is sent by the AS to stop the mission. The ROS bridge will forward the signal to the operator which in turn will stop the ROS bridge and finish the run. fs_msgs/FinishedSignal","title":"Subscribed topics"},{"location":"ros-bridge/#services","text":"/fsds/reset fs_msgs/Reset Resets car to start location.","title":"Services"},{"location":"ros-bridge/#units","text":"If a topic streams a standard ROS message (like sensor_msgs/Imu ) then the units will be the recommended units in the message documentation. Custom messages (from the fs_msgs package) use the units specified in the message documentation as well. If in doubt, interpret distances in meters, angles in radians and rates in m/s and rad/s, etc.","title":"Units"},{"location":"ros-bridge/#coordinate-frames-and-transforms","text":"The primary frame is the fsds/FSCar frame, which is fixed at the center of the car following the ROS coordinate system convention . The center of the car is the Unreal Engine car pawn position, which in turn is also the center of gravity. The ROS bridge regularly publishes static transforms between the fsds/FSCar frame and each of the cameras and lidars. Naming of these frames is fsds/SENSORNAME . For example, a lidar named Example will publish it's points in the fsds/Example frame. The position and orientation of a camera named Test will become available in the frame /fsds/Test . The transforms published on the /tf_static topic are the transforms specified in the settings.json . Only static transforms within the vehicle are published. All positions and rotations published by the ROS bridge are in line with the ROS defaults . This is the same coordinate system as everything else within the simulator .","title":"Coordinate frames and transforms"},{"location":"ros-bridge/#parameters","text":"/fsds/ros_bridge/update_gps_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10hz). Timer callback frequency for updating and publishing the gps sensordata. This value must be equal or higher to the update frequency of the sensor configured in the settings.json /fsds/ros_bridge/update_imu_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the imu messages. This value must be equal or higher to the minimual sample rate of the sensor configured in the settings.json /fsds/ros_bridge/update_gss_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.01 seconds (100hz). Timer callback frequency for updating and publishing the ground speed sensor messages. /fsds/ros_bridge/update_odom_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.004 seconds (250hz). Timer callback frequency for updating and publishing the odometry. /fsds/ros_bridge/publish_static_tf_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 1 seconds (1 hz). The frequency at which the static transforms are published. /fsds/ros_bridge/update_lidar_every_n_sec [double] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: 0.1 seconds (10 hz). The frequency at which the lidar is publshed. /fsds/ros_bridge/competition_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false , during competition set to true If competition mode is enabled, the testing_only topics won't be available. /fsds/ros_bridge/manual_mode [bool] Set in: $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false Do not enable vehicle api control. You can control the car using the keyboard in the simulator. /fsds/ros_bridge/UDP_control [bool] Set in $(fsds_ros_bridge)/launch/fsds_ros_bridge.launch Default: false Force UDP connection for the fsds/controlCommand topic when running the fsds_ros_bridge in WSL","title":"Parameters"},{"location":"ros-bridge/#visualization","text":"This package contains some useful launch and config files which will help you in visualizing the data being streamed through the above topics. To open Rviz with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch rviz:=true . To open Multiplot with this configuration file, run roslaunch fsds_ros_bridge fsds_ros_bridge.launch plot:=true .","title":"Visualization"},{"location":"ros-bridge/#monitoring","text":"Performance monitoring of the ROS Bridge is described here","title":"Monitoring"},{"location":"software-install-instructions/","text":"Required software installation instructions This page helps you install software required for running and developing the simulator. You probably only need parts of it. The other guides in this documentation will tell you what you need. Install Unreal Engine (Windows) Ensure your windows user has no special characters! If it contains special characters there will be a conflict with your Win10 user folder name, and mess up unreal engine reading the directory path. Go to unrealengine.com and download the epic installer. You need an account for this. Install the epic installer. Launch the epic installer and install Unreal Engine 4.25 Install Unreal Engine (Ubuntu) This project uses Unreal Engine 4.25. Before you can use unreal engine on Ubuntu, you must register at unrealengine.com and get access to the UnrealEngine github repo. After you get access, clone the repo and build it: git clone --depth=1 -b 4.25 https://github.com/EpicGames/UnrealEngine.git cd UnrealEngine ./Setup.sh && ./GenerateProjectFiles.sh && make Run it by executing ./Engine/Binaries/Linux/UE4Editor . The Unreal Editor should now open without errors. Install visual studio 2019 (Windows) Download visual studio 2019 (community edition) During installation, choose the following components: Desktop development with C++ Game development with C++ Linux development with C++ At 'Invidual Components select: C++ CMake tools for Windows Windows 10 SDK 10.0.18.362.0 .NET Framework 4.7 SDK Windows Subsystem for Linux (WSL) In case you want to run the ros bridge on Windows, you will need Windows Subsystem for Linux Enable Windows Subsystem for Linux 1 (WSL1) . No need to update to version 2. Install ubuntu 18.04 LTS. If you are on windows server, enable windows susbsystem for linux in the server manager and install ubuntu . Install ROS Melodic/Noetic/Galactic (Ubuntu / WSL) Follow the guide on their website And add the following line to end of your ~/.bashrc file: source /opt/ros/melodic/setup.bash source ~/Formula-Student-Driverless-Simulator/ros/devel/setup.bash Be sure to replace melodic with your specific version Gui applications from WSL (Xming) By default, if you are running Windows Subsystem for Linux with Ubuntu, you can't run gui applications. This is super annoying if you want to use rqt applicatoins like rviz or rqt_plot. It is easy to get this working though! Just install Xming on windows, and run it. Next, go into the Ubuntu terminal and run export DISPLAY=:0 . Now you can run any all them gui apps! You can even add export DISPLAY=:0 to your ~/.bashrc to always be able to use gui apps without having to manually run export.","title":"Software install instructions"},{"location":"software-install-instructions/#required-software-installation-instructions","text":"This page helps you install software required for running and developing the simulator. You probably only need parts of it. The other guides in this documentation will tell you what you need.","title":"Required software installation instructions"},{"location":"software-install-instructions/#install-unreal-engine-windows","text":"Ensure your windows user has no special characters! If it contains special characters there will be a conflict with your Win10 user folder name, and mess up unreal engine reading the directory path. Go to unrealengine.com and download the epic installer. You need an account for this. Install the epic installer. Launch the epic installer and install Unreal Engine 4.25","title":"Install Unreal Engine (Windows)"},{"location":"software-install-instructions/#install-unreal-engine-ubuntu","text":"This project uses Unreal Engine 4.25. Before you can use unreal engine on Ubuntu, you must register at unrealengine.com and get access to the UnrealEngine github repo. After you get access, clone the repo and build it: git clone --depth=1 -b 4.25 https://github.com/EpicGames/UnrealEngine.git cd UnrealEngine ./Setup.sh && ./GenerateProjectFiles.sh && make Run it by executing ./Engine/Binaries/Linux/UE4Editor . The Unreal Editor should now open without errors.","title":"Install Unreal Engine (Ubuntu)"},{"location":"software-install-instructions/#install-visual-studio-2019-windows","text":"Download visual studio 2019 (community edition) During installation, choose the following components: Desktop development with C++ Game development with C++ Linux development with C++ At 'Invidual Components select: C++ CMake tools for Windows Windows 10 SDK 10.0.18.362.0 .NET Framework 4.7 SDK","title":"Install visual studio 2019 (Windows)"},{"location":"software-install-instructions/#windows-subsystem-for-linux-wsl","text":"In case you want to run the ros bridge on Windows, you will need Windows Subsystem for Linux Enable Windows Subsystem for Linux 1 (WSL1) . No need to update to version 2. Install ubuntu 18.04 LTS. If you are on windows server, enable windows susbsystem for linux in the server manager and install ubuntu .","title":"Windows Subsystem for Linux (WSL)"},{"location":"software-install-instructions/#install-ros-melodicnoeticgalactic-ubuntu-wsl","text":"Follow the guide on their website And add the following line to end of your ~/.bashrc file: source /opt/ros/melodic/setup.bash source ~/Formula-Student-Driverless-Simulator/ros/devel/setup.bash Be sure to replace melodic with your specific version","title":"Install ROS Melodic/Noetic/Galactic (Ubuntu / WSL)"},{"location":"software-install-instructions/#gui-applications-from-wsl-xming","text":"By default, if you are running Windows Subsystem for Linux with Ubuntu, you can't run gui applications. This is super annoying if you want to use rqt applicatoins like rviz or rqt_plot. It is easy to get this working though! Just install Xming on windows, and run it. Next, go into the Ubuntu terminal and run export DISPLAY=:0 . Now you can run any all them gui apps! You can even add export DISPLAY=:0 to your ~/.bashrc to always be able to use gui apps without having to manually run export.","title":"Gui applications from WSL (Xming)"},{"location":"spectator/","text":"Spectator The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Spectators are not able to interact with the world Launching the server A simulator launched as 'server' will accept external viewers (spectators) to join the game. You can launch the simulation as a server in the main menu by pressing the 'TODO' button. The server will run on port 7777 port, make sure spectators can connect to it. In most cases you will need to add a firewall rule to allow TCP and UDP traffic. When behind a router you might need to do some port forwarding. To skip the menu and launch the game as server directly, you can use the following command: FSDS.exe /Game/TrainingMap?listen This opens the TrainingMap and allows external clients (spectators) to connect. Within the settings.json you can configure the server password. Using the password is usefull to prevent trolls and curious people from connecting to the simulator and breaking the simulator. { \"SpectatorPassword\": \"password\", ... If the password is not configured in the settings.json, the password is set to password . At this moment it is not possible to start a server without password. Launching the spectator Spectators can enter the ip of the server in the main menu to connect to a running simulator. Multiple simultaneous spectators in a single world should work as well. If a spectator loses connection to the server it will go back to the main menu. To skip the menu and launch the spectator directly, you can use the following command: FSDS.exe 0.0.0.0?password=123456 Where 0.0.0.0 is replaced by the external ip of the server and 123456 is replaced by the server password. If you enter a wrong password you are taken back to the main menu. No error will be shown. Using the spectator When starting the spectator it will be launched in follow-car mode. In this mode the camera will always point at the vehicle. When the vehicle crosses a triggerline, the camera will move to another viewpoint. You can toggle the follow-car mode using the F key on your keyboard. When not following the car you can use the wsdaqe keys to move around and your mouse to look around. Adding spectator points to the map Spectator viewpoints are places in the world where the spectator can be teleported to. These locations are defined in the map and cannot be changed during the simulation. To create a new viewpoint, add a new CameraActor to the world and place it wherever you like. Next, add a AirsimSpectatorTeleportTrigger to the world. In the settings, set the camera to the one you just created. Whenever the vehicle touches this trigger object, the spectator will be teleported to the selected camera (in follow-car mode). Running the spectator from Unreal Editor During development you can use the following steps to run the spectator directly from the unreal editor, skipping the main menu: In your settings.json , set the SpectatorServerPassword to an empty string like so: \"SpectatorServerPassword\": \"\" . In the run configuration, set the number of players to 2 and Net Mode to Play As Listen Server . Play. Both play in selected viewport and play as standalone game work.","title":"Spectator"},{"location":"spectator/#spectator","text":"The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Spectators are not able to interact with the world","title":"Spectator"},{"location":"spectator/#launching-the-server","text":"A simulator launched as 'server' will accept external viewers (spectators) to join the game. You can launch the simulation as a server in the main menu by pressing the 'TODO' button. The server will run on port 7777 port, make sure spectators can connect to it. In most cases you will need to add a firewall rule to allow TCP and UDP traffic. When behind a router you might need to do some port forwarding. To skip the menu and launch the game as server directly, you can use the following command: FSDS.exe /Game/TrainingMap?listen This opens the TrainingMap and allows external clients (spectators) to connect. Within the settings.json you can configure the server password. Using the password is usefull to prevent trolls and curious people from connecting to the simulator and breaking the simulator. { \"SpectatorPassword\": \"password\", ... If the password is not configured in the settings.json, the password is set to password . At this moment it is not possible to start a server without password.","title":"Launching the server"},{"location":"spectator/#launching-the-spectator","text":"Spectators can enter the ip of the server in the main menu to connect to a running simulator. Multiple simultaneous spectators in a single world should work as well. If a spectator loses connection to the server it will go back to the main menu. To skip the menu and launch the spectator directly, you can use the following command: FSDS.exe 0.0.0.0?password=123456 Where 0.0.0.0 is replaced by the external ip of the server and 123456 is replaced by the server password. If you enter a wrong password you are taken back to the main menu. No error will be shown.","title":"Launching the spectator"},{"location":"spectator/#using-the-spectator","text":"When starting the spectator it will be launched in follow-car mode. In this mode the camera will always point at the vehicle. When the vehicle crosses a triggerline, the camera will move to another viewpoint. You can toggle the follow-car mode using the F key on your keyboard. When not following the car you can use the wsdaqe keys to move around and your mouse to look around.","title":"Using the spectator"},{"location":"spectator/#adding-spectator-points-to-the-map","text":"Spectator viewpoints are places in the world where the spectator can be teleported to. These locations are defined in the map and cannot be changed during the simulation. To create a new viewpoint, add a new CameraActor to the world and place it wherever you like. Next, add a AirsimSpectatorTeleportTrigger to the world. In the settings, set the camera to the one you just created. Whenever the vehicle touches this trigger object, the spectator will be teleported to the selected camera (in follow-car mode).","title":"Adding spectator points to the map"},{"location":"spectator/#running-the-spectator-from-unreal-editor","text":"During development you can use the following steps to run the spectator directly from the unreal editor, skipping the main menu: In your settings.json , set the SpectatorServerPassword to an empty string like so: \"SpectatorServerPassword\": \"\" . In the run configuration, set the number of players to 2 and Net Mode to Play As Listen Server . Play. Both play in selected viewport and play as standalone game work.","title":"Running the spectator from Unreal Editor"},{"location":"statistics/","text":"ROS Bridge Monitoring ROS Bridge and Rpc performance monitoring is done by the Statistics (/ros/src/fsds_ros_bridge/include/statistics.h) class. This class is used by the AirsimROSWrapper class to monitor latency of rpc calls as well as the frequency of ceratain topics in the ROS network. The statistics gathered by the methods of this class and temporarily stored by private class members will be printed live at about 1Hz. This will be useful both for development of the simulator as well as for ensuring fairness in the competition, where a team will be allowed to retry a run if problems are diagnosed on the simulator side. The following describes how the class and the auxiliary classes Timer and ROSMsgCounter measure performance and are implemented, so that as a developer you can monitor new publishers, subscribers, rpc calls, or actually any line of code in the AirSimROSWrapper class. We are gathering statistics about: Rpc calls: getGpsData getCarState getImuData (vector) simGetImages (vector) getLidarData (vector) setCarControls ROS publishing frequency of the following publishers: odom_local_ned_pub global_gps_pub cam_pub_vec_ (vector) lidar_pub_vec_ (vector) imu_pub_vec_ (vector) ROS callback frequency of the following subscriber(s): control_cmd_sub There will be one instance of this class for each Rpc call to be monitored. To make a latency measurement, the appropriate instance pointer has to be passed to a new Timer class (see below): ros_bridge::Statistics rpcCallStatistics = ros_bridge::Statistics(\"rpcCallName\"); { // Enter scope to be timed ros_bridge::Timer timer(&rpcCallStatistics); // Start timing // do the Rpc Call } // Go out of scope -> call the timer destructor // which automatically stores the time elapsed in the instance of // the class that was passed There will also be an instance of this class for each ROS publisher/subscriber. To count a new incoming or outgoing message the simple construct below can be used: ros_bridge::Statistics pubSubStatistics = ros_bridge::Statistics(\"pubSubName\");; // For a publisher: { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); pubSub.publish(data); } // For a subscriber: void callback(msg) { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); // Do something with msg } // scope ends, destructor is called and count is incremented for // the Statistics object In the 1Hz ROS timer, the Print function will be called (the wrapper which applies this action to all the instances) followed by the Reset function (the wrapper which applies this action to all the instances) which ensures that counters are set to 0 and that vectors of durations (latencies) are emptied.","title":"ROS Bridge statistics"},{"location":"statistics/#ros-bridge-monitoring","text":"ROS Bridge and Rpc performance monitoring is done by the Statistics (/ros/src/fsds_ros_bridge/include/statistics.h) class. This class is used by the AirsimROSWrapper class to monitor latency of rpc calls as well as the frequency of ceratain topics in the ROS network. The statistics gathered by the methods of this class and temporarily stored by private class members will be printed live at about 1Hz. This will be useful both for development of the simulator as well as for ensuring fairness in the competition, where a team will be allowed to retry a run if problems are diagnosed on the simulator side. The following describes how the class and the auxiliary classes Timer and ROSMsgCounter measure performance and are implemented, so that as a developer you can monitor new publishers, subscribers, rpc calls, or actually any line of code in the AirSimROSWrapper class. We are gathering statistics about: Rpc calls: getGpsData getCarState getImuData (vector) simGetImages (vector) getLidarData (vector) setCarControls ROS publishing frequency of the following publishers: odom_local_ned_pub global_gps_pub cam_pub_vec_ (vector) lidar_pub_vec_ (vector) imu_pub_vec_ (vector) ROS callback frequency of the following subscriber(s): control_cmd_sub There will be one instance of this class for each Rpc call to be monitored. To make a latency measurement, the appropriate instance pointer has to be passed to a new Timer class (see below): ros_bridge::Statistics rpcCallStatistics = ros_bridge::Statistics(\"rpcCallName\"); { // Enter scope to be timed ros_bridge::Timer timer(&rpcCallStatistics); // Start timing // do the Rpc Call } // Go out of scope -> call the timer destructor // which automatically stores the time elapsed in the instance of // the class that was passed There will also be an instance of this class for each ROS publisher/subscriber. To count a new incoming or outgoing message the simple construct below can be used: ros_bridge::Statistics pubSubStatistics = ros_bridge::Statistics(\"pubSubName\");; // For a publisher: { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); pubSub.publish(data); } // For a subscriber: void callback(msg) { // pass persistent Statistics object (by reference) ros_bridge::ROSMsgCounter counter(&pubSubStatistics); // Do something with msg } // scope ends, destructor is called and count is incremented for // the Statistics object In the 1Hz ROS timer, the Print function will be called (the wrapper which applies this action to all the instances) followed by the Reset function (the wrapper which applies this action to all the instances) which ensures that counters are set to 0 and that vectors of durations (latencies) are emptied.","title":"ROS Bridge Monitoring"},{"location":"system-overview/","text":"This document is intended to read top-to-bottom. Do yourself a favour and read the whole thing without skipping ;) Formula Student Driverless Simulation: System overview FSDS is built around Unreal Engine 4 (the game engine) and the AirSim plugin. The game engine does all the graphical rendering, collision simulation and car movement simulation. Autonomous systems connect to the simulation using ROS topics provided by the ros-bridge. A separate component - the operator - will provide an interface to officials to controll the simulation during competition. Autonomous systems Every Autonomous System (AS) will run on its separated environment. Ideally, this would be separate virtual machines but also Docker containers could be used. The ASs are expected to continuously run a ROS master. When the simulator is ready to do a drive for the given AS it will launch a ROS node connected to this AS's ROS master. This ROS node will exist outside of the AS's environment. Instead, it will run on the simulator computer. The simulator ROS node will publish sensor data and listen for car setpoints on a set of topics defined here . When the simulation is finished, the ROS node will disconnect from the AS's ROS master and sensor data stops coming in. During the competition, the teams will not be allowed to access their ASs. All remote access to the environments will be cut off completely. So every AS must be able to run multiple missions without human interference. To let the simulation know what is expected from it, the simulator will send signal messages a few seconds before the event start. These messages contain information about the mission (trackdrive, autocross, etc). When the AS receives a mission message it can expect to receive sensor data shortly after. Since the lap timer for all events start whenever the car crosses the start-line, the ASs can take all the time they need to launch their relevant algorithms for the mission (within a reasonable time). The Operator The operator is a continuously running program that is like the spider in the web. The operator offers a web interface to the event's officials. Using this web interface the officials can choose which team/car is going to drive and on which track. The official will also be able to send the start signal, view lap times, down or out cones, car off course's. There is also an emergency stop button in case the car is uncontrollable. Only 1 car at the time will be able to run on this AS. The official can select which team, and thus which AS, is currently selected. When the team changes, not only the AS but also the car inside the virtual world will change. The sensor suite (sensor types and locations, defined in a custom settings.json file) and the car livery (looks of the car) are updated. The operator keeps track of these details and passes them along to the virtual world to ensure accurate representation. When the operator wants to connect an AS to the simulated world, it launches the ROS bridge. Read more about the ROS bridge below. If the operator wants to disconnect the AS from the simulated world it stops the bridge node and the connections are stopped. During a mission, the operator keeps polling the world for 'referee state'. This is information that in a physical world would be relevant to the referee. Currently, this includes a list of down or out cones and the timestamp of when they went down or out, a list of lap times and a list of when the car went off-track. More information will for sure be added. What happens inside the simulation is stored in a single logbook. This includes all referee updates, which ASs were selected and which tracks were used. If something unexpected went wrong like a system's crash or error, a short description of what happened is included in this logbook. It gives a timeline of everything that happened to always go back afterwards and check what happened. The logbook is stored on disk so that in the event of a whole system crash we will still have the logbook. It is also shown within the referee's web interface. You can run the simulation stack yourself perfectly fine without the operator. The operator is just a tool for officials to easely manage the competition. The ROS Bridge The ROS bridge node connects to the simulated world inside Unreal Engine using AirSim (more on that later). On the one hand, it requests sensor data and passes it along on ROS topics to the current AS. On the other hand, it receives car control commands from the AS and forwards it to the virtual world. So it acts as a bridge between the two. The node that is launched pointing at the AS's ROS master so that it can publish and subscribe to topics within the AS. Physically this node runs on the server where the Unreal world is being simulated. The node is launched by the operator. When the operator launches the ROS bridge it passes along some mission variables. This includes mission type (trackdrive or autocross) and information about how it can use data collected in previous runs. For example, first the AS will receive \"autocross on track A\" and it knows it cannot use any previous collected information. Then it receives \"trackdrive on track A\" and it knows it can use data collected in the first autocross drive to go faster. It is the responsibilty of the teams to detect when they are 'done'. After the required number of laps, the car has to come to a full stop. If the AS wants to store things (like track information), this is the time to wrap those up. In case of a successfull run the official will instruct the operator to stop the ROS bridge and the AS won't receive sensor data anymore. When the official presses the emergency brake, the connection between the ROS bridge is stopped immediately and the operator will send one last car setpoint to make the car come to a stop. Thers is no 'stop' signal from the simulator to the AS. At this point only ROS is supported, at this moment there are no plans to support other technologies. The virtual world (Unreal Engine and AirSim) The actual simulation takes place inside an Unreal Engine 4 world. Unreal takes care of the heavy lifting involved with a real-life simulation. All physics, lighting and world-building are handled by Unreal. AirSim is used to connect Unreal with the operator and ROS bridge. This plugin is added inside the Unreal world and takes control of most of the game logic. It receives the sensor suite and simulates the sensors, it moves the car according to trajectory setpoints and exposes an RPC API for external management. This RPC API is used by the simulator to interact with the world. The Spectator The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Read more about usage of the spectator here .","title":"System overview"},{"location":"system-overview/#formula-student-driverless-simulation-system-overview","text":"FSDS is built around Unreal Engine 4 (the game engine) and the AirSim plugin. The game engine does all the graphical rendering, collision simulation and car movement simulation. Autonomous systems connect to the simulation using ROS topics provided by the ros-bridge. A separate component - the operator - will provide an interface to officials to controll the simulation during competition.","title":"Formula Student Driverless Simulation: System overview"},{"location":"system-overview/#autonomous-systems","text":"Every Autonomous System (AS) will run on its separated environment. Ideally, this would be separate virtual machines but also Docker containers could be used. The ASs are expected to continuously run a ROS master. When the simulator is ready to do a drive for the given AS it will launch a ROS node connected to this AS's ROS master. This ROS node will exist outside of the AS's environment. Instead, it will run on the simulator computer. The simulator ROS node will publish sensor data and listen for car setpoints on a set of topics defined here . When the simulation is finished, the ROS node will disconnect from the AS's ROS master and sensor data stops coming in. During the competition, the teams will not be allowed to access their ASs. All remote access to the environments will be cut off completely. So every AS must be able to run multiple missions without human interference. To let the simulation know what is expected from it, the simulator will send signal messages a few seconds before the event start. These messages contain information about the mission (trackdrive, autocross, etc). When the AS receives a mission message it can expect to receive sensor data shortly after. Since the lap timer for all events start whenever the car crosses the start-line, the ASs can take all the time they need to launch their relevant algorithms for the mission (within a reasonable time).","title":"Autonomous systems"},{"location":"system-overview/#the-operator","text":"The operator is a continuously running program that is like the spider in the web. The operator offers a web interface to the event's officials. Using this web interface the officials can choose which team/car is going to drive and on which track. The official will also be able to send the start signal, view lap times, down or out cones, car off course's. There is also an emergency stop button in case the car is uncontrollable. Only 1 car at the time will be able to run on this AS. The official can select which team, and thus which AS, is currently selected. When the team changes, not only the AS but also the car inside the virtual world will change. The sensor suite (sensor types and locations, defined in a custom settings.json file) and the car livery (looks of the car) are updated. The operator keeps track of these details and passes them along to the virtual world to ensure accurate representation. When the operator wants to connect an AS to the simulated world, it launches the ROS bridge. Read more about the ROS bridge below. If the operator wants to disconnect the AS from the simulated world it stops the bridge node and the connections are stopped. During a mission, the operator keeps polling the world for 'referee state'. This is information that in a physical world would be relevant to the referee. Currently, this includes a list of down or out cones and the timestamp of when they went down or out, a list of lap times and a list of when the car went off-track. More information will for sure be added. What happens inside the simulation is stored in a single logbook. This includes all referee updates, which ASs were selected and which tracks were used. If something unexpected went wrong like a system's crash or error, a short description of what happened is included in this logbook. It gives a timeline of everything that happened to always go back afterwards and check what happened. The logbook is stored on disk so that in the event of a whole system crash we will still have the logbook. It is also shown within the referee's web interface. You can run the simulation stack yourself perfectly fine without the operator. The operator is just a tool for officials to easely manage the competition.","title":"The Operator"},{"location":"system-overview/#the-ros-bridge","text":"The ROS bridge node connects to the simulated world inside Unreal Engine using AirSim (more on that later). On the one hand, it requests sensor data and passes it along on ROS topics to the current AS. On the other hand, it receives car control commands from the AS and forwards it to the virtual world. So it acts as a bridge between the two. The node that is launched pointing at the AS's ROS master so that it can publish and subscribe to topics within the AS. Physically this node runs on the server where the Unreal world is being simulated. The node is launched by the operator. When the operator launches the ROS bridge it passes along some mission variables. This includes mission type (trackdrive or autocross) and information about how it can use data collected in previous runs. For example, first the AS will receive \"autocross on track A\" and it knows it cannot use any previous collected information. Then it receives \"trackdrive on track A\" and it knows it can use data collected in the first autocross drive to go faster. It is the responsibilty of the teams to detect when they are 'done'. After the required number of laps, the car has to come to a full stop. If the AS wants to store things (like track information), this is the time to wrap those up. In case of a successfull run the official will instruct the operator to stop the ROS bridge and the AS won't receive sensor data anymore. When the official presses the emergency brake, the connection between the ROS bridge is stopped immediately and the operator will send one last car setpoint to make the car come to a stop. Thers is no 'stop' signal from the simulator to the AS. At this point only ROS is supported, at this moment there are no plans to support other technologies.","title":"The ROS Bridge"},{"location":"system-overview/#the-virtual-world-unreal-engine-and-airsim","text":"The actual simulation takes place inside an Unreal Engine 4 world. Unreal takes care of the heavy lifting involved with a real-life simulation. All physics, lighting and world-building are handled by Unreal. AirSim is used to connect Unreal with the operator and ROS bridge. This plugin is added inside the Unreal world and takes control of most of the game logic. It receives the sensor suite and simulates the sensors, it moves the car according to trajectory setpoints and exposes an RPC API for external management. This RPC API is used by the simulator to interact with the world.","title":"The virtual world (Unreal Engine and AirSim)"},{"location":"system-overview/#the-spectator","text":"The spectator provides an eye into the virtual world from an external computer. Through a network connection the virtual world state is replicated and shown on the screen. The user can operate the camera with its mouse and keyboard. Read more about usage of the spectator here .","title":"The Spectator"},{"location":"vehicle_model/","text":"Vehicle Dynamic model One of the most controversial subjects of any simulator is the vehicle dynamic model. This is the piece of the simulation that actually changes the state of the vehicle. In building this simulator for the FSOnline competition, our design philosophy was the following: All teams will use the same vehicle dynamic model . We are well aware that all teams have put effort into developing dynamic models of their own FSCar for controls and simulation purposes. However, we want the FSOnline DV Dynamic event to purely be a battle of autonomous software. Even if this will require teams to tweak their path planning and control algorithms, it will make sure that the winner of this event is truly the team that can take any race car and push it to its limits the most. The dynamic model will have a high enough fidelity such that it is virtually impossible to overfit to it/reverse the plant or run open loop . This will force teams to use system identification techniques similar to the ones that are used on a real car and no cheating or unfair advantage will be given to any teams. A third-party, open-source model would be ideal . This way, not even the developers of the simulation (Formula Student Team Delft) would have an edge over other teams. Everyone has access to the same code and has had no experience working with it or been involved in developing it. Unreal Engine provides the PhysXVehicles that was developed by Nvidia. This seemed like the perfect solution for our simulator, given that it complies with all the criteria of our design philosophy above. Airsim simply interacts with the PhysXCar API in these files. Vehicle pawn center Just like everything inside the simulated world, the vehicle has a position within the world (x, y, z). Relative to this point the rest of the vehicle is constructed. The collision box, the center of gravity and the sensor positions all are relative to this center point. The center position is located at the bottom of the car and on the horizontal plane in the geometric center. When car is stationary the height offset between the world track and the pawn position is 0. In below image the blue dot represents the location of the pawn center: Vehicle Layout Overview Vehicle collision model The collision model (bounding box) defines which parts of the car interact with other parts of the world. The default vehicle (TechnionCarPawn) has the following bounding boxes: Width: 100cm Length: 180cm Height: 50cm The bounding boxes will be centered ontop of the vehicle pawn center. In the Vehicle Layout Overview picture above the orange lines show the bounding box. These boundries are unrealted to the vehicle 3d model. The car may look smaller or bigger but they will hit cones all the same. Vehicle center of gravity The center of gravity of the car sits in the center of the collision model. The center of gravity of the default vehicle (TechnionCarPawn) is 25cm above the vehicle pawn center. In turn, when the car is stationary, the height offset between the world ground and the center of gravity is 25 cm. Within the Vehicle Layout Overview picture the red dot represents the center of gravity. More properties of competition vehicles Mass: 255 kg Max speed: ~27 m/s Drag coefficient: 0.3 [-] How to configure vehicle properties? All vehicle proprties are set in the unreal vehicle pawn classes. You can view and edit these configuratoin files using the Unreal Editor in /UE4Project/Plugins/AirSim/Content/VehicleAdv/ .","title":"Vehicle model"},{"location":"vehicle_model/#vehicle-dynamic-model","text":"One of the most controversial subjects of any simulator is the vehicle dynamic model. This is the piece of the simulation that actually changes the state of the vehicle. In building this simulator for the FSOnline competition, our design philosophy was the following: All teams will use the same vehicle dynamic model . We are well aware that all teams have put effort into developing dynamic models of their own FSCar for controls and simulation purposes. However, we want the FSOnline DV Dynamic event to purely be a battle of autonomous software. Even if this will require teams to tweak their path planning and control algorithms, it will make sure that the winner of this event is truly the team that can take any race car and push it to its limits the most. The dynamic model will have a high enough fidelity such that it is virtually impossible to overfit to it/reverse the plant or run open loop . This will force teams to use system identification techniques similar to the ones that are used on a real car and no cheating or unfair advantage will be given to any teams. A third-party, open-source model would be ideal . This way, not even the developers of the simulation (Formula Student Team Delft) would have an edge over other teams. Everyone has access to the same code and has had no experience working with it or been involved in developing it. Unreal Engine provides the PhysXVehicles that was developed by Nvidia. This seemed like the perfect solution for our simulator, given that it complies with all the criteria of our design philosophy above. Airsim simply interacts with the PhysXCar API in these files.","title":"Vehicle Dynamic model"},{"location":"vehicle_model/#vehicle-pawn-center","text":"Just like everything inside the simulated world, the vehicle has a position within the world (x, y, z). Relative to this point the rest of the vehicle is constructed. The collision box, the center of gravity and the sensor positions all are relative to this center point. The center position is located at the bottom of the car and on the horizontal plane in the geometric center. When car is stationary the height offset between the world track and the pawn position is 0. In below image the blue dot represents the location of the pawn center: Vehicle Layout Overview","title":"Vehicle pawn center"},{"location":"vehicle_model/#vehicle-collision-model","text":"The collision model (bounding box) defines which parts of the car interact with other parts of the world. The default vehicle (TechnionCarPawn) has the following bounding boxes: Width: 100cm Length: 180cm Height: 50cm The bounding boxes will be centered ontop of the vehicle pawn center. In the Vehicle Layout Overview picture above the orange lines show the bounding box. These boundries are unrealted to the vehicle 3d model. The car may look smaller or bigger but they will hit cones all the same.","title":"Vehicle collision model"},{"location":"vehicle_model/#vehicle-center-of-gravity","text":"The center of gravity of the car sits in the center of the collision model. The center of gravity of the default vehicle (TechnionCarPawn) is 25cm above the vehicle pawn center. In turn, when the car is stationary, the height offset between the world ground and the center of gravity is 25 cm. Within the Vehicle Layout Overview picture the red dot represents the center of gravity.","title":"Vehicle center of gravity"},{"location":"vehicle_model/#more-properties-of-competition-vehicles","text":"Mass: 255 kg Max speed: ~27 m/s Drag coefficient: 0.3 [-]","title":"More properties of competition vehicles"},{"location":"vehicle_model/#how-to-configure-vehicle-properties","text":"All vehicle proprties are set in the unreal vehicle pawn classes. You can view and edit these configuratoin files using the Unreal Editor in /UE4Project/Plugins/AirSim/Content/VehicleAdv/ .","title":"How to configure vehicle properties?"}]}